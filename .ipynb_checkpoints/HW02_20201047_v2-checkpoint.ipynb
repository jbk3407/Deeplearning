{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HW02\n",
    "Deep Learning, GIST RT5101-01, 2021, Spring, (Tue/Thurs 2:30~3:45)\n",
    "***\n",
    "\n",
    "\n",
    "How to submit your homework\n",
    "Submit your jupyter notebook file with the filename of HW02_studentnumber.ipynb on GEL\n",
    "Ex) HW02_20184021.ipynb\n",
    "\n",
    "Submission deadline\n",
    "2021.05.12, Wednesday 23:59 (PM)\n",
    "\n",
    "Plagiarism\n",
    "We encourage you to discuss this homework with your friends or TA, but you should write your own code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1. (Build simple fully connected layer model)\n",
    "- step 1. Build your simple FCN layer by using fully connected layer\n",
    "- step 2. train your own model\n",
    "- Do not use coding lecture sample model connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "height, width, ch = 32, 32, 3\n",
    "data_dir = '/home/chan/PycharmProjects/CIFAR10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleFCN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SimpleFCN, self).__init__()\n",
    "        self.fc1 = nn.Linear(height * width * ch, 2048)\n",
    "        self.fc2 = nn.Linear(2048, 2048)\n",
    "        self.fc3 = nn.Linear(2048, 2048)\n",
    "        self.fc4 = nn.Linear(2048, 1024)\n",
    "        self.fc5 = nn.Linear(1024, 1024)\n",
    "        self.fc6 = nn.Linear(1024, 512)\n",
    "        self.fc7 = nn.Linear(512, 512)\n",
    "        self.fc8 = nn.Linear(512, 256)\n",
    "        self.fc9 = nn.Linear(256, num_classes)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = F.relu(self.fc6(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc7(x))\n",
    "        x = F.relu(self.fc8(x))\n",
    "        x = self.dropout(x)\n",
    "        output = self.fc9(x)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2. (dataset code part)\n",
    "- Load your dataset from dataset folder\n",
    "- using rgb dataset ex) cifar10, cifar100\n",
    "- you can use custom dataset from web site ex) dog vs cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "def dataset(is_train):\n",
    "    if is_train:\n",
    "        transform = [\n",
    "            transforms.RandomCrop((height, width), padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=(0.5,), std=(0.5,))\n",
    "        ]\n",
    "        subdir = '/train'\n",
    "    \n",
    "    else:\n",
    "        transform = [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=(0.5,), std=(0.5,))\n",
    "        ]\n",
    "        subdir = '/test'\n",
    "        \n",
    "    dataset = datasets.ImageFolder(root=data_dir + subdir, transform=transforms.Compose(transform))\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][0/391]\tLoss 2.3011 (2.3011)\n",
      "Epoch: [0][100/391]\tLoss 2.3031 (2.3031)\n",
      "Epoch: [0][200/391]\tLoss 2.3035 (2.3030)\n",
      "Epoch: [0][300/391]\tLoss 2.3021 (2.3030)\n",
      "train result: Loss: 2.3029338252258302, Acc: 9.652\n",
      "\n",
      "Test: [0/79]\tLoss 2.3091 (2.3091)\tPrec@1 0.000 (0.000)\n",
      "Validation result: Loss: 2.3026607963562014, Acc: 10.0\n",
      "\n",
      "Epoch: [1][0/391]\tLoss 2.3028 (2.3028)\n",
      "Epoch: [1][100/391]\tLoss 2.3000 (2.3027)\n",
      "Epoch: [1][200/391]\tLoss 2.3028 (2.3028)\n",
      "Epoch: [1][300/391]\tLoss 2.3022 (2.3028)\n",
      "train result: Loss: 2.3028155332946776, Acc: 9.9\n",
      "\n",
      "Test: [0/79]\tLoss 2.2787 (2.2787)\tPrec@1 100.000 (100.000)\n",
      "Validation result: Loss: 2.3025735877990723, Acc: 10.0\n",
      "\n",
      "Epoch: [2][0/391]\tLoss 2.3035 (2.3035)\n",
      "Epoch: [2][100/391]\tLoss 2.3039 (2.3029)\n",
      "Epoch: [2][200/391]\tLoss 2.3043 (2.3028)\n",
      "Epoch: [2][300/391]\tLoss 2.3027 (2.3028)\n",
      "train result: Loss: 2.30279047416687, Acc: 9.82\n",
      "\n",
      "Test: [0/79]\tLoss 2.2634 (2.2634)\tPrec@1 100.000 (100.000)\n",
      "Validation result: Loss: 2.3025964042663576, Acc: 10.0\n",
      "\n",
      "Epoch: [3][0/391]\tLoss 2.2985 (2.2985)\n",
      "Epoch: [3][100/391]\tLoss 2.3034 (2.3029)\n",
      "Epoch: [3][200/391]\tLoss 2.3022 (2.3028)\n",
      "Epoch: [3][300/391]\tLoss 2.3039 (2.3028)\n",
      "train result: Loss: 2.3026939279174803, Acc: 10.05\n",
      "\n",
      "Test: [0/79]\tLoss 2.2824 (2.2824)\tPrec@1 100.000 (100.000)\n",
      "Validation result: Loss: 2.302315549468994, Acc: 10.0\n",
      "\n",
      "Epoch: [4][0/391]\tLoss 2.3016 (2.3016)\n",
      "Epoch: [4][100/391]\tLoss 2.3005 (2.3025)\n",
      "Epoch: [4][200/391]\tLoss 2.3010 (2.3024)\n",
      "Epoch: [4][300/391]\tLoss 2.3034 (2.3024)\n",
      "train result: Loss: 2.302353607940674, Acc: 10.502\n",
      "\n",
      "Test: [0/79]\tLoss 2.3059 (2.3059)\tPrec@1 0.000 (0.000)\n",
      "Validation result: Loss: 2.301944473648071, Acc: 10.0\n",
      "\n",
      "Epoch: [5][0/391]\tLoss 2.2972 (2.2972)\n",
      "Epoch: [5][100/391]\tLoss 2.3018 (2.3019)\n",
      "Epoch: [5][200/391]\tLoss 2.3009 (2.3017)\n",
      "Epoch: [5][300/391]\tLoss 2.3020 (2.3012)\n",
      "train result: Loss: 2.300213314971924, Acc: 12.22\n",
      "\n",
      "Test: [0/79]\tLoss 2.2979 (2.2979)\tPrec@1 0.000 (0.000)\n",
      "Validation result: Loss: 2.2939461696624757, Acc: 16.7\n",
      "\n",
      "Epoch: [6][0/391]\tLoss 2.2901 (2.2901)\n",
      "Epoch: [6][100/391]\tLoss 2.2583 (2.2825)\n",
      "Epoch: [6][200/391]\tLoss 2.1441 (2.2307)\n",
      "Epoch: [6][300/391]\tLoss 2.0765 (2.1791)\n",
      "train result: Loss: 2.1493612838745118, Acc: 18.034\n",
      "\n",
      "Test: [0/79]\tLoss 1.9954 (1.9954)\tPrec@1 0.000 (0.000)\n",
      "Validation result: Loss: 2.031596426010132, Acc: 19.8\n",
      "\n",
      "Epoch: [7][0/391]\tLoss 2.0073 (2.0073)\n",
      "Epoch: [7][100/391]\tLoss 1.9567 (2.0315)\n",
      "Epoch: [7][200/391]\tLoss 1.9441 (2.0221)\n",
      "Epoch: [7][300/391]\tLoss 1.9660 (2.0193)\n",
      "train result: Loss: 2.0133691236114504, Acc: 20.012\n",
      "\n",
      "Test: [0/79]\tLoss 2.0202 (2.0202)\tPrec@1 0.000 (0.000)\n",
      "Validation result: Loss: 1.9758726697921754, Acc: 20.33\n",
      "\n",
      "Epoch: [8][0/391]\tLoss 1.9779 (1.9779)\n",
      "Epoch: [8][100/391]\tLoss 1.9944 (1.9917)\n",
      "Epoch: [8][200/391]\tLoss 1.9742 (1.9867)\n",
      "Epoch: [8][300/391]\tLoss 1.9945 (1.9830)\n",
      "train result: Loss: 1.9785742490386964, Acc: 20.534\n",
      "\n",
      "Test: [0/79]\tLoss 2.0720 (2.0720)\tPrec@1 0.000 (0.000)\n",
      "Validation result: Loss: 1.9293401775360108, Acc: 22.36\n",
      "\n",
      "Epoch: [9][0/391]\tLoss 1.8609 (1.8609)\n",
      "Epoch: [9][100/391]\tLoss 1.9406 (1.9604)\n",
      "Epoch: [9][200/391]\tLoss 1.9680 (1.9558)\n",
      "Epoch: [9][300/391]\tLoss 1.8925 (1.9481)\n",
      "train result: Loss: 1.9411299505996704, Acc: 22.712\n",
      "\n",
      "Test: [0/79]\tLoss 2.2317 (2.2317)\tPrec@1 4.688 (4.688)\n",
      "Validation result: Loss: 1.8624070346832275, Acc: 25.1\n",
      "\n",
      "Epoch: [10][0/391]\tLoss 1.9649 (1.9649)\n",
      "Epoch: [10][100/391]\tLoss 1.8844 (1.8994)\n",
      "Epoch: [10][200/391]\tLoss 1.8635 (1.8823)\n",
      "Epoch: [10][300/391]\tLoss 1.8441 (1.8641)\n",
      "train result: Loss: 1.8522028665924073, Acc: 28.066\n",
      "\n",
      "Test: [0/79]\tLoss 1.9425 (1.9425)\tPrec@1 25.781 (25.781)\n",
      "Validation result: Loss: 1.7721880350112915, Acc: 30.05\n",
      "\n",
      "Epoch: [11][0/391]\tLoss 1.8270 (1.8270)\n",
      "Epoch: [11][100/391]\tLoss 1.7977 (1.8086)\n",
      "Epoch: [11][200/391]\tLoss 1.7871 (1.7967)\n",
      "Epoch: [11][300/391]\tLoss 1.6443 (1.7916)\n",
      "train result: Loss: 1.7880388355255128, Acc: 31.11\n",
      "\n",
      "Test: [0/79]\tLoss 1.7614 (1.7614)\tPrec@1 43.750 (43.750)\n",
      "Validation result: Loss: 1.7077417762756348, Acc: 35.18\n",
      "\n",
      "Epoch: [12][0/391]\tLoss 1.6692 (1.6692)\n",
      "Epoch: [12][100/391]\tLoss 1.8120 (1.7623)\n",
      "Epoch: [12][200/391]\tLoss 1.6925 (1.7534)\n",
      "Epoch: [12][300/391]\tLoss 1.7061 (1.7502)\n",
      "train result: Loss: 1.7452534576034546, Acc: 33.25\n",
      "\n",
      "Test: [0/79]\tLoss 1.8068 (1.8068)\tPrec@1 35.156 (35.156)\n",
      "Validation result: Loss: 1.677320058631897, Acc: 36.37\n",
      "\n",
      "Epoch: [13][0/391]\tLoss 1.7236 (1.7236)\n",
      "Epoch: [13][100/391]\tLoss 1.6946 (1.7278)\n",
      "Epoch: [13][200/391]\tLoss 1.6866 (1.7331)\n",
      "Epoch: [13][300/391]\tLoss 1.7152 (1.7244)\n",
      "train result: Loss: 1.7190230899429322, Acc: 35.232\n",
      "\n",
      "Test: [0/79]\tLoss 1.6439 (1.6439)\tPrec@1 46.094 (46.094)\n",
      "Validation result: Loss: 1.6445026363372803, Acc: 38.45\n",
      "\n",
      "Epoch: [14][0/391]\tLoss 1.5555 (1.5555)\n",
      "Epoch: [14][100/391]\tLoss 1.9250 (1.7052)\n",
      "Epoch: [14][200/391]\tLoss 1.8166 (1.6987)\n",
      "Epoch: [14][300/391]\tLoss 1.6765 (1.6918)\n",
      "train result: Loss: 1.6945321695709228, Acc: 36.566\n",
      "\n",
      "Test: [0/79]\tLoss 1.7295 (1.7295)\tPrec@1 42.188 (42.188)\n",
      "Validation result: Loss: 1.6353837715148927, Acc: 39.56\n",
      "\n",
      "Epoch: [15][0/391]\tLoss 1.7205 (1.7205)\n",
      "Epoch: [15][100/391]\tLoss 1.6043 (1.6755)\n",
      "Epoch: [15][200/391]\tLoss 1.8592 (1.6815)\n",
      "Epoch: [15][300/391]\tLoss 1.6675 (1.6732)\n",
      "train result: Loss: 1.6728968462753295, Acc: 37.618\n",
      "\n",
      "Test: [0/79]\tLoss 1.4657 (1.4657)\tPrec@1 53.125 (53.125)\n",
      "Validation result: Loss: 1.6255816375732421, Acc: 39.22\n",
      "\n",
      "Epoch: [16][0/391]\tLoss 1.6128 (1.6128)\n",
      "Epoch: [16][100/391]\tLoss 1.5702 (1.6702)\n",
      "Epoch: [16][200/391]\tLoss 1.6557 (1.6623)\n",
      "Epoch: [16][300/391]\tLoss 1.6201 (1.6565)\n",
      "train result: Loss: 1.6540665999221802, Acc: 38.75\n",
      "\n",
      "Test: [0/79]\tLoss 1.5144 (1.5144)\tPrec@1 47.656 (47.656)\n",
      "Validation result: Loss: 1.5951022287368775, Acc: 40.86\n",
      "\n",
      "Epoch: [17][0/391]\tLoss 1.7564 (1.7564)\n",
      "Epoch: [17][100/391]\tLoss 1.5969 (1.6459)\n",
      "Epoch: [17][200/391]\tLoss 1.7362 (1.6422)\n",
      "Epoch: [17][300/391]\tLoss 1.6149 (1.6427)\n",
      "train result: Loss: 1.6384758501815795, Acc: 39.518\n",
      "\n",
      "Test: [0/79]\tLoss 1.7139 (1.7139)\tPrec@1 43.750 (43.750)\n",
      "Validation result: Loss: 1.583419644355774, Acc: 41.15\n",
      "\n",
      "Epoch: [18][0/391]\tLoss 1.5261 (1.5261)\n",
      "Epoch: [18][100/391]\tLoss 1.5517 (1.6261)\n",
      "Epoch: [18][200/391]\tLoss 1.5857 (1.6318)\n",
      "Epoch: [18][300/391]\tLoss 1.7093 (1.6275)\n",
      "train result: Loss: 1.624656181564331, Acc: 40.318\n",
      "\n",
      "Test: [0/79]\tLoss 1.5386 (1.5386)\tPrec@1 49.219 (49.219)\n",
      "Validation result: Loss: 1.57288873462677, Acc: 42.9\n",
      "\n",
      "Epoch: [19][0/391]\tLoss 1.7288 (1.7288)\n",
      "Epoch: [19][100/391]\tLoss 1.7078 (1.6155)\n",
      "Epoch: [19][200/391]\tLoss 1.8404 (1.6139)\n",
      "Epoch: [19][300/391]\tLoss 1.5520 (1.6146)\n",
      "train result: Loss: 1.6144149216079713, Acc: 40.826\n",
      "\n",
      "Test: [0/79]\tLoss 1.5497 (1.5497)\tPrec@1 49.219 (49.219)\n",
      "Validation result: Loss: 1.5666852073669433, Acc: 42.77\n",
      "\n",
      "Epoch: [20][0/391]\tLoss 1.5049 (1.5049)\n",
      "Epoch: [20][100/391]\tLoss 1.6636 (1.6029)\n",
      "Epoch: [20][200/391]\tLoss 1.5693 (1.6029)\n",
      "Epoch: [20][300/391]\tLoss 1.5263 (1.5980)\n",
      "train result: Loss: 1.5985129918289185, Acc: 41.606\n",
      "\n",
      "Test: [0/79]\tLoss 1.4248 (1.4248)\tPrec@1 46.094 (46.094)\n",
      "Validation result: Loss: 1.5898223413467407, Acc: 41.71\n",
      "\n",
      "Epoch: [21][0/391]\tLoss 1.5497 (1.5497)\n",
      "Epoch: [21][100/391]\tLoss 1.7770 (1.5757)\n",
      "Epoch: [21][200/391]\tLoss 1.5770 (1.5783)\n",
      "Epoch: [21][300/391]\tLoss 1.6476 (1.5806)\n",
      "train result: Loss: 1.5804124405670166, Acc: 42.574\n",
      "\n",
      "Test: [0/79]\tLoss 1.5499 (1.5499)\tPrec@1 53.125 (53.125)\n",
      "Validation result: Loss: 1.5454596982955933, Acc: 43.99\n",
      "\n",
      "Epoch: [22][0/391]\tLoss 1.3128 (1.3128)\n",
      "Epoch: [22][100/391]\tLoss 1.6010 (1.5424)\n",
      "Epoch: [22][200/391]\tLoss 1.4932 (1.5445)\n",
      "Epoch: [22][300/391]\tLoss 1.4539 (1.5415)\n",
      "train result: Loss: 1.5351072862243653, Acc: 43.67\n",
      "\n",
      "Test: [0/79]\tLoss 1.5357 (1.5357)\tPrec@1 46.875 (46.875)\n",
      "Validation result: Loss: 1.4847695405960084, Acc: 46.15\n",
      "\n",
      "Epoch: [23][0/391]\tLoss 1.5603 (1.5603)\n",
      "Epoch: [23][100/391]\tLoss 1.5423 (1.5269)\n",
      "Epoch: [23][200/391]\tLoss 1.5094 (1.5213)\n",
      "Epoch: [23][300/391]\tLoss 1.5043 (1.5177)\n",
      "train result: Loss: 1.5153655094528198, Acc: 44.368\n",
      "\n",
      "Test: [0/79]\tLoss 1.5479 (1.5479)\tPrec@1 47.656 (47.656)\n",
      "Validation result: Loss: 1.458547876739502, Acc: 47.18\n",
      "\n",
      "Epoch: [24][0/391]\tLoss 1.3737 (1.3737)\n",
      "Epoch: [24][100/391]\tLoss 1.5656 (1.4964)\n",
      "Epoch: [24][200/391]\tLoss 1.3804 (1.5067)\n",
      "Epoch: [24][300/391]\tLoss 1.3484 (1.4967)\n",
      "train result: Loss: 1.496709094657898, Acc: 45.138\n",
      "\n",
      "Test: [0/79]\tLoss 1.4781 (1.4781)\tPrec@1 49.219 (49.219)\n",
      "Validation result: Loss: 1.4304062463760376, Acc: 47.96\n",
      "\n",
      "Epoch: [25][0/391]\tLoss 1.5637 (1.5637)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [25][100/391]\tLoss 1.6159 (1.4916)\n",
      "Epoch: [25][200/391]\tLoss 1.5697 (1.4923)\n",
      "Epoch: [25][300/391]\tLoss 1.4775 (1.4857)\n",
      "train result: Loss: 1.485699347000122, Acc: 45.798\n",
      "\n",
      "Test: [0/79]\tLoss 1.3591 (1.3591)\tPrec@1 53.125 (53.125)\n",
      "Validation result: Loss: 1.4580339702606202, Acc: 46.94\n",
      "\n",
      "Epoch: [26][0/391]\tLoss 1.5041 (1.5041)\n",
      "Epoch: [26][100/391]\tLoss 1.6195 (1.4689)\n",
      "Epoch: [26][200/391]\tLoss 1.3986 (1.4751)\n",
      "Epoch: [26][300/391]\tLoss 1.3436 (1.4710)\n",
      "train result: Loss: 1.4707455673599243, Acc: 46.572\n",
      "\n",
      "Test: [0/79]\tLoss 1.2931 (1.2931)\tPrec@1 52.344 (52.344)\n",
      "Validation result: Loss: 1.4206155296325684, Acc: 48.1\n",
      "\n",
      "Epoch: [27][0/391]\tLoss 1.5172 (1.5172)\n",
      "Epoch: [27][100/391]\tLoss 1.5469 (1.4529)\n",
      "Epoch: [27][200/391]\tLoss 1.3974 (1.4506)\n",
      "Epoch: [27][300/391]\tLoss 1.4706 (1.4508)\n",
      "train result: Loss: 1.4550787614059448, Acc: 47.284\n",
      "\n",
      "Test: [0/79]\tLoss 1.2385 (1.2385)\tPrec@1 57.031 (57.031)\n",
      "Validation result: Loss: 1.4322804418563844, Acc: 47.87\n",
      "\n",
      "Epoch: [28][0/391]\tLoss 1.5587 (1.5587)\n",
      "Epoch: [28][100/391]\tLoss 1.4608 (1.4490)\n",
      "Epoch: [28][200/391]\tLoss 1.4289 (1.4500)\n",
      "Epoch: [28][300/391]\tLoss 1.5016 (1.4463)\n",
      "train result: Loss: 1.4463765955352783, Acc: 47.446\n",
      "\n",
      "Test: [0/79]\tLoss 1.3358 (1.3358)\tPrec@1 53.125 (53.125)\n",
      "Validation result: Loss: 1.4127309816360474, Acc: 48.98\n",
      "\n",
      "Epoch: [29][0/391]\tLoss 1.4304 (1.4304)\n",
      "Epoch: [29][100/391]\tLoss 1.5992 (1.4421)\n",
      "Epoch: [29][200/391]\tLoss 1.3516 (1.4470)\n",
      "Epoch: [29][300/391]\tLoss 1.5064 (1.4351)\n",
      "train result: Loss: 1.4357778290939331, Acc: 48.106\n",
      "\n",
      "Test: [0/79]\tLoss 1.1260 (1.1260)\tPrec@1 59.375 (59.375)\n",
      "Validation result: Loss: 1.4261444480895995, Acc: 48.11\n",
      "\n",
      "Epoch: [30][0/391]\tLoss 1.5328 (1.5328)\n",
      "Epoch: [30][100/391]\tLoss 1.4054 (1.4197)\n",
      "Epoch: [30][200/391]\tLoss 1.5408 (1.4226)\n",
      "Epoch: [30][300/391]\tLoss 1.4251 (1.4234)\n",
      "train result: Loss: 1.4228859548187256, Acc: 48.752\n",
      "\n",
      "Test: [0/79]\tLoss 1.3923 (1.3923)\tPrec@1 50.781 (50.781)\n",
      "Validation result: Loss: 1.3943400859832764, Acc: 50.06\n",
      "\n",
      "Epoch: [31][0/391]\tLoss 1.3532 (1.3532)\n",
      "Epoch: [31][100/391]\tLoss 1.4139 (1.4264)\n",
      "Epoch: [31][200/391]\tLoss 1.3383 (1.4238)\n",
      "Epoch: [31][300/391]\tLoss 1.4495 (1.4183)\n",
      "train result: Loss: 1.41704272315979, Acc: 48.856\n",
      "\n",
      "Test: [0/79]\tLoss 1.3668 (1.3668)\tPrec@1 52.344 (52.344)\n",
      "Validation result: Loss: 1.3932416889190673, Acc: 50.51\n",
      "\n",
      "Epoch: [32][0/391]\tLoss 1.4458 (1.4458)\n",
      "Epoch: [32][100/391]\tLoss 1.5711 (1.4097)\n",
      "Epoch: [32][200/391]\tLoss 1.5263 (1.4024)\n",
      "Epoch: [32][300/391]\tLoss 1.3584 (1.4035)\n",
      "train result: Loss: 1.403732283782959, Acc: 49.552\n",
      "\n",
      "Test: [0/79]\tLoss 1.6172 (1.6172)\tPrec@1 47.656 (47.656)\n",
      "Validation result: Loss: 1.3689751749038697, Acc: 50.5\n",
      "\n",
      "Epoch: [33][0/391]\tLoss 1.4075 (1.4075)\n",
      "Epoch: [33][100/391]\tLoss 1.4658 (1.3865)\n",
      "Epoch: [33][200/391]\tLoss 1.4024 (1.3959)\n",
      "Epoch: [33][300/391]\tLoss 1.5626 (1.3974)\n",
      "train result: Loss: 1.3933241053009033, Acc: 49.904\n",
      "\n",
      "Test: [0/79]\tLoss 1.5313 (1.5313)\tPrec@1 50.000 (50.000)\n",
      "Validation result: Loss: 1.378187837600708, Acc: 50.43\n",
      "\n",
      "Epoch: [34][0/391]\tLoss 1.4246 (1.4246)\n",
      "Epoch: [34][100/391]\tLoss 1.3195 (1.3930)\n",
      "Epoch: [34][200/391]\tLoss 1.4963 (1.3921)\n",
      "Epoch: [34][300/391]\tLoss 1.2741 (1.3889)\n",
      "train result: Loss: 1.392275295944214, Acc: 50.15\n",
      "\n",
      "Test: [0/79]\tLoss 1.5815 (1.5815)\tPrec@1 42.969 (42.969)\n",
      "Validation result: Loss: 1.363621199607849, Acc: 51.21\n",
      "\n",
      "Epoch: [35][0/391]\tLoss 1.3733 (1.3733)\n",
      "Epoch: [35][100/391]\tLoss 1.3853 (1.3772)\n",
      "Epoch: [35][200/391]\tLoss 1.2850 (1.3798)\n",
      "Epoch: [35][300/391]\tLoss 1.2381 (1.3835)\n",
      "train result: Loss: 1.3806789317703247, Acc: 50.384\n",
      "\n",
      "Test: [0/79]\tLoss 1.3762 (1.3762)\tPrec@1 54.688 (54.688)\n",
      "Validation result: Loss: 1.3682537755966186, Acc: 51.62\n",
      "\n",
      "Epoch: [36][0/391]\tLoss 1.3278 (1.3278)\n",
      "Epoch: [36][100/391]\tLoss 1.2746 (1.3609)\n",
      "Epoch: [36][200/391]\tLoss 1.3698 (1.3659)\n",
      "Epoch: [36][300/391]\tLoss 1.5779 (1.3709)\n",
      "train result: Loss: 1.3731456211853028, Acc: 51.036\n",
      "\n",
      "Test: [0/79]\tLoss 1.6599 (1.6599)\tPrec@1 43.750 (43.750)\n",
      "Validation result: Loss: 1.367069743537903, Acc: 52.2\n",
      "\n",
      "Epoch: [37][0/391]\tLoss 1.4240 (1.4240)\n",
      "Epoch: [37][100/391]\tLoss 1.4636 (1.3546)\n",
      "Epoch: [37][200/391]\tLoss 1.5274 (1.3624)\n",
      "Epoch: [37][300/391]\tLoss 1.3636 (1.3653)\n",
      "train result: Loss: 1.3693726043319703, Acc: 51.002\n",
      "\n",
      "Test: [0/79]\tLoss 1.5577 (1.5577)\tPrec@1 47.656 (47.656)\n",
      "Validation result: Loss: 1.3625952541351318, Acc: 51.61\n",
      "\n",
      "Epoch: [38][0/391]\tLoss 1.3987 (1.3987)\n",
      "Epoch: [38][100/391]\tLoss 1.1549 (1.3495)\n",
      "Epoch: [38][200/391]\tLoss 1.3582 (1.3642)\n",
      "Epoch: [38][300/391]\tLoss 1.2727 (1.3571)\n",
      "train result: Loss: 1.3593484104156495, Acc: 51.582\n",
      "\n",
      "Test: [0/79]\tLoss 1.4095 (1.4095)\tPrec@1 50.000 (50.000)\n",
      "Validation result: Loss: 1.3428820404052735, Acc: 52.27\n",
      "\n",
      "Epoch: [39][0/391]\tLoss 1.2552 (1.2552)\n",
      "Epoch: [39][100/391]\tLoss 1.4986 (1.3583)\n",
      "Epoch: [39][200/391]\tLoss 1.4823 (1.3509)\n",
      "Epoch: [39][300/391]\tLoss 1.2196 (1.3521)\n",
      "train result: Loss: 1.351891951904297, Acc: 51.884\n",
      "\n",
      "Test: [0/79]\tLoss 1.7980 (1.7980)\tPrec@1 39.062 (39.062)\n",
      "Validation result: Loss: 1.3553805307388305, Acc: 51.61\n",
      "\n",
      "Epoch: [40][0/391]\tLoss 1.3596 (1.3596)\n",
      "Epoch: [40][100/391]\tLoss 1.2715 (1.3281)\n",
      "Epoch: [40][200/391]\tLoss 1.2450 (1.3418)\n",
      "Epoch: [40][300/391]\tLoss 1.1103 (1.3427)\n",
      "train result: Loss: 1.3461751680374145, Acc: 52.158\n",
      "\n",
      "Test: [0/79]\tLoss 1.3323 (1.3323)\tPrec@1 53.125 (53.125)\n",
      "Validation result: Loss: 1.3375902463912963, Acc: 52.25\n",
      "\n",
      "Epoch: [41][0/391]\tLoss 1.3956 (1.3956)\n",
      "Epoch: [41][100/391]\tLoss 1.2938 (1.3205)\n",
      "Epoch: [41][200/391]\tLoss 1.4222 (1.3412)\n",
      "Epoch: [41][300/391]\tLoss 1.5219 (1.3387)\n",
      "train result: Loss: 1.3391350917053222, Acc: 52.094\n",
      "\n",
      "Test: [0/79]\tLoss 1.4462 (1.4462)\tPrec@1 55.469 (55.469)\n",
      "Validation result: Loss: 1.3242550988197326, Acc: 53.18\n",
      "\n",
      "Epoch: [42][0/391]\tLoss 1.4092 (1.4092)\n",
      "Epoch: [42][100/391]\tLoss 1.2772 (1.3341)\n",
      "Epoch: [42][200/391]\tLoss 1.4404 (1.3289)\n",
      "Epoch: [42][300/391]\tLoss 1.3753 (1.3240)\n",
      "train result: Loss: 1.3324407803726197, Acc: 52.392\n",
      "\n",
      "Test: [0/79]\tLoss 1.1954 (1.1954)\tPrec@1 60.156 (60.156)\n",
      "Validation result: Loss: 1.3298031021118164, Acc: 52.13\n",
      "\n",
      "Epoch: [43][0/391]\tLoss 1.3243 (1.3243)\n",
      "Epoch: [43][100/391]\tLoss 1.3073 (1.3256)\n",
      "Epoch: [43][200/391]\tLoss 1.1873 (1.3182)\n",
      "Epoch: [43][300/391]\tLoss 1.2421 (1.3233)\n",
      "train result: Loss: 1.3248779013824463, Acc: 53.118\n",
      "\n",
      "Test: [0/79]\tLoss 1.2885 (1.2885)\tPrec@1 55.469 (55.469)\n",
      "Validation result: Loss: 1.3208756298065185, Acc: 53.76\n",
      "\n",
      "Epoch: [44][0/391]\tLoss 1.3647 (1.3647)\n",
      "Epoch: [44][100/391]\tLoss 1.3494 (1.3368)\n",
      "Epoch: [44][200/391]\tLoss 1.1625 (1.3234)\n",
      "Epoch: [44][300/391]\tLoss 1.3526 (1.3221)\n",
      "train result: Loss: 1.3181874626922607, Acc: 53.084\n",
      "\n",
      "Test: [0/79]\tLoss 1.5985 (1.5985)\tPrec@1 46.875 (46.875)\n",
      "Validation result: Loss: 1.3302912740707398, Acc: 54.07\n",
      "\n",
      "Epoch: [45][0/391]\tLoss 1.5131 (1.5131)\n",
      "Epoch: [45][100/391]\tLoss 1.3544 (1.3027)\n",
      "Epoch: [45][200/391]\tLoss 1.4134 (1.3156)\n",
      "Epoch: [45][300/391]\tLoss 1.5246 (1.3115)\n",
      "train result: Loss: 1.3161008679962158, Acc: 53.378\n",
      "\n",
      "Test: [0/79]\tLoss 1.5218 (1.5218)\tPrec@1 49.219 (49.219)\n",
      "Validation result: Loss: 1.311567211341858, Acc: 53.68\n",
      "\n",
      "Epoch: [46][0/391]\tLoss 1.2866 (1.2866)\n",
      "Epoch: [46][100/391]\tLoss 1.3404 (1.2961)\n",
      "Epoch: [46][200/391]\tLoss 1.2102 (1.3010)\n",
      "Epoch: [46][300/391]\tLoss 1.2797 (1.3036)\n",
      "train result: Loss: 1.3089588731765747, Acc: 53.792\n",
      "\n",
      "Test: [0/79]\tLoss 1.2027 (1.2027)\tPrec@1 57.812 (57.812)\n",
      "Validation result: Loss: 1.3277780498504639, Acc: 53.23\n",
      "\n",
      "Epoch: [47][0/391]\tLoss 1.2636 (1.2636)\n",
      "Epoch: [47][100/391]\tLoss 1.3505 (1.2952)\n",
      "Epoch: [47][200/391]\tLoss 1.3778 (1.2955)\n",
      "Epoch: [47][300/391]\tLoss 1.4169 (1.2979)\n",
      "train result: Loss: 1.3001793993377686, Acc: 54.176\n",
      "\n",
      "Test: [0/79]\tLoss 1.5074 (1.5074)\tPrec@1 50.781 (50.781)\n",
      "Validation result: Loss: 1.3354049176216125, Acc: 53.14\n",
      "\n",
      "Epoch: [48][0/391]\tLoss 1.2719 (1.2719)\n",
      "Epoch: [48][100/391]\tLoss 1.3867 (1.2944)\n",
      "Epoch: [48][200/391]\tLoss 1.3964 (1.2979)\n",
      "Epoch: [48][300/391]\tLoss 1.2150 (1.2945)\n",
      "train result: Loss: 1.3001543354415894, Acc: 54.11\n",
      "\n",
      "Test: [0/79]\tLoss 1.5492 (1.5492)\tPrec@1 47.656 (47.656)\n",
      "Validation result: Loss: 1.3143726358413697, Acc: 53.7\n",
      "\n",
      "Epoch: [49][0/391]\tLoss 1.2413 (1.2413)\n",
      "Epoch: [49][100/391]\tLoss 1.0681 (1.2873)\n",
      "Epoch: [49][200/391]\tLoss 1.2641 (1.2938)\n",
      "Epoch: [49][300/391]\tLoss 1.2113 (1.2897)\n",
      "train result: Loss: 1.2901040224456788, Acc: 54.506\n",
      "\n",
      "Test: [0/79]\tLoss 1.3208 (1.3208)\tPrec@1 54.688 (54.688)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation result: Loss: 1.3004630458831787, Acc: 54.4\n",
      "\n",
      "Epoch: [50][0/391]\tLoss 1.2991 (1.2991)\n",
      "Epoch: [50][100/391]\tLoss 1.2079 (1.2734)\n",
      "Epoch: [50][200/391]\tLoss 1.3742 (1.2870)\n",
      "Epoch: [50][300/391]\tLoss 1.3611 (1.2867)\n",
      "train result: Loss: 1.2858324786758424, Acc: 54.596\n",
      "\n",
      "Test: [0/79]\tLoss 1.5939 (1.5939)\tPrec@1 50.000 (50.000)\n",
      "Validation result: Loss: 1.2971073070526122, Acc: 54.77\n",
      "\n",
      "Epoch: [51][0/391]\tLoss 1.2782 (1.2782)\n",
      "Epoch: [51][100/391]\tLoss 1.3147 (1.2781)\n",
      "Epoch: [51][200/391]\tLoss 1.4076 (1.2865)\n",
      "Epoch: [51][300/391]\tLoss 1.3568 (1.2839)\n",
      "train result: Loss: 1.2833746755981446, Acc: 54.842\n",
      "\n",
      "Test: [0/79]\tLoss 1.6453 (1.6453)\tPrec@1 46.094 (46.094)\n",
      "Validation result: Loss: 1.3044737604141234, Acc: 54.89\n",
      "\n",
      "Epoch: [52][0/391]\tLoss 1.3249 (1.3249)\n",
      "Epoch: [52][100/391]\tLoss 1.3153 (1.2625)\n",
      "Epoch: [52][200/391]\tLoss 1.2125 (1.2712)\n",
      "Epoch: [52][300/391]\tLoss 1.3229 (1.2740)\n",
      "train result: Loss: 1.2754943839645385, Acc: 54.936\n",
      "\n",
      "Test: [0/79]\tLoss 1.4174 (1.4174)\tPrec@1 53.125 (53.125)\n",
      "Validation result: Loss: 1.283664674949646, Acc: 54.93\n",
      "\n",
      "Epoch: [53][0/391]\tLoss 1.2666 (1.2666)\n",
      "Epoch: [53][100/391]\tLoss 1.3563 (1.2574)\n",
      "Epoch: [53][200/391]\tLoss 1.4796 (1.2694)\n",
      "Epoch: [53][300/391]\tLoss 1.2132 (1.2680)\n",
      "train result: Loss: 1.2717469924163818, Acc: 55.0\n",
      "\n",
      "Test: [0/79]\tLoss 1.4150 (1.4150)\tPrec@1 53.906 (53.906)\n",
      "Validation result: Loss: 1.2946161582946778, Acc: 54.92\n",
      "\n",
      "Epoch: [54][0/391]\tLoss 1.2958 (1.2958)\n",
      "Epoch: [54][100/391]\tLoss 1.1776 (1.2596)\n",
      "Epoch: [54][200/391]\tLoss 1.2805 (1.2581)\n",
      "Epoch: [54][300/391]\tLoss 1.2078 (1.2638)\n",
      "train result: Loss: 1.2678369110488892, Acc: 55.38\n",
      "\n",
      "Test: [0/79]\tLoss 1.3198 (1.3198)\tPrec@1 56.250 (56.250)\n",
      "Validation result: Loss: 1.2760607429504394, Acc: 55.75\n",
      "\n",
      "Epoch: [55][0/391]\tLoss 1.2647 (1.2647)\n",
      "Epoch: [55][100/391]\tLoss 1.4125 (1.2557)\n",
      "Epoch: [55][200/391]\tLoss 1.3331 (1.2524)\n",
      "Epoch: [55][300/391]\tLoss 1.2394 (1.2588)\n",
      "train result: Loss: 1.2668937481307982, Acc: 55.684\n",
      "\n",
      "Test: [0/79]\tLoss 1.4418 (1.4418)\tPrec@1 50.781 (50.781)\n",
      "Validation result: Loss: 1.2946010194778443, Acc: 54.66\n",
      "\n",
      "Epoch: [56][0/391]\tLoss 1.2121 (1.2121)\n",
      "Epoch: [56][100/391]\tLoss 1.4935 (1.2356)\n",
      "Epoch: [56][200/391]\tLoss 1.1311 (1.2455)\n",
      "Epoch: [56][300/391]\tLoss 1.3513 (1.2514)\n",
      "train result: Loss: 1.257722788734436, Acc: 55.996\n",
      "\n",
      "Test: [0/79]\tLoss 1.2476 (1.2476)\tPrec@1 57.812 (57.812)\n",
      "Validation result: Loss: 1.2665629755020142, Acc: 55.57\n",
      "\n",
      "Epoch: [57][0/391]\tLoss 1.2334 (1.2334)\n",
      "Epoch: [57][100/391]\tLoss 1.1939 (1.2383)\n",
      "Epoch: [57][200/391]\tLoss 1.2501 (1.2444)\n",
      "Epoch: [57][300/391]\tLoss 1.2058 (1.2469)\n",
      "train result: Loss: 1.2530713800048827, Acc: 55.704\n",
      "\n",
      "Test: [0/79]\tLoss 1.1987 (1.1987)\tPrec@1 60.938 (60.938)\n",
      "Validation result: Loss: 1.296462045288086, Acc: 55.1\n",
      "\n",
      "Epoch: [58][0/391]\tLoss 1.1955 (1.1955)\n",
      "Epoch: [58][100/391]\tLoss 1.1467 (1.2416)\n",
      "Epoch: [58][200/391]\tLoss 1.0268 (1.2375)\n",
      "Epoch: [58][300/391]\tLoss 1.2666 (1.2453)\n",
      "train result: Loss: 1.2504233195495607, Acc: 56.292\n",
      "\n",
      "Test: [0/79]\tLoss 1.4529 (1.4529)\tPrec@1 54.688 (54.688)\n",
      "Validation result: Loss: 1.2831016579627992, Acc: 55.52\n",
      "\n",
      "Epoch: [59][0/391]\tLoss 1.2762 (1.2762)\n",
      "Epoch: [59][100/391]\tLoss 1.3389 (1.2466)\n",
      "Epoch: [59][200/391]\tLoss 1.0971 (1.2507)\n",
      "Epoch: [59][300/391]\tLoss 1.1637 (1.2449)\n",
      "train result: Loss: 1.2428912602615356, Acc: 56.582\n",
      "\n",
      "Test: [0/79]\tLoss 1.5581 (1.5581)\tPrec@1 50.781 (50.781)\n",
      "Validation result: Loss: 1.2870272417068482, Acc: 54.72\n",
      "\n",
      "Epoch: [60][0/391]\tLoss 1.0526 (1.0526)\n",
      "Epoch: [60][100/391]\tLoss 1.2492 (1.2381)\n",
      "Epoch: [60][200/391]\tLoss 1.2287 (1.2397)\n",
      "Epoch: [60][300/391]\tLoss 1.4000 (1.2409)\n",
      "train result: Loss: 1.2436549906158447, Acc: 56.518\n",
      "\n",
      "Test: [0/79]\tLoss 1.3506 (1.3506)\tPrec@1 60.156 (60.156)\n",
      "Validation result: Loss: 1.2620925313949585, Acc: 55.9\n",
      "\n",
      "Epoch: [61][0/391]\tLoss 1.4356 (1.4356)\n",
      "Epoch: [61][100/391]\tLoss 1.3030 (1.2268)\n",
      "Epoch: [61][200/391]\tLoss 1.1502 (1.2327)\n",
      "Epoch: [61][300/391]\tLoss 1.2380 (1.2336)\n",
      "train result: Loss: 1.2346504133987426, Acc: 56.674\n",
      "\n",
      "Test: [0/79]\tLoss 1.4529 (1.4529)\tPrec@1 55.469 (55.469)\n",
      "Validation result: Loss: 1.264795908164978, Acc: 55.99\n",
      "\n",
      "Epoch: [62][0/391]\tLoss 1.2719 (1.2719)\n",
      "Epoch: [62][100/391]\tLoss 1.1298 (1.2311)\n",
      "Epoch: [62][200/391]\tLoss 1.2818 (1.2375)\n",
      "Epoch: [62][300/391]\tLoss 1.1786 (1.2333)\n",
      "train result: Loss: 1.2289825782775878, Acc: 57.122\n",
      "\n",
      "Test: [0/79]\tLoss 1.2091 (1.2091)\tPrec@1 60.938 (60.938)\n",
      "Validation result: Loss: 1.254786477279663, Acc: 56.71\n",
      "\n",
      "Epoch: [63][0/391]\tLoss 1.2314 (1.2314)\n",
      "Epoch: [63][100/391]\tLoss 1.1475 (1.2153)\n",
      "Epoch: [63][200/391]\tLoss 1.1384 (1.2136)\n",
      "Epoch: [63][300/391]\tLoss 1.3048 (1.2246)\n",
      "train result: Loss: 1.2248342755889892, Acc: 57.126\n",
      "\n",
      "Test: [0/79]\tLoss 1.4429 (1.4429)\tPrec@1 52.344 (52.344)\n",
      "Validation result: Loss: 1.2580589403152467, Acc: 55.89\n",
      "\n",
      "Epoch: [64][0/391]\tLoss 1.0966 (1.0966)\n",
      "Epoch: [64][100/391]\tLoss 1.2414 (1.2166)\n",
      "Epoch: [64][200/391]\tLoss 1.2709 (1.2194)\n",
      "Epoch: [64][300/391]\tLoss 1.1363 (1.2183)\n",
      "train result: Loss: 1.223896764945984, Acc: 57.146\n",
      "\n",
      "Test: [0/79]\tLoss 1.3811 (1.3811)\tPrec@1 57.031 (57.031)\n",
      "Validation result: Loss: 1.2673256870269776, Acc: 55.47\n",
      "\n",
      "Epoch: [65][0/391]\tLoss 1.1976 (1.1976)\n",
      "Epoch: [65][100/391]\tLoss 1.2973 (1.2103)\n",
      "Epoch: [65][200/391]\tLoss 1.1937 (1.2079)\n",
      "Epoch: [65][300/391]\tLoss 1.0804 (1.2166)\n",
      "train result: Loss: 1.2209790354537964, Acc: 57.278\n",
      "\n",
      "Test: [0/79]\tLoss 1.3392 (1.3392)\tPrec@1 53.906 (53.906)\n",
      "Validation result: Loss: 1.261637352180481, Acc: 56.0\n",
      "\n",
      "Epoch: [66][0/391]\tLoss 1.2326 (1.2326)\n",
      "Epoch: [66][100/391]\tLoss 1.2159 (1.2112)\n",
      "Epoch: [66][200/391]\tLoss 1.1935 (1.2181)\n",
      "Epoch: [66][300/391]\tLoss 1.1854 (1.2156)\n",
      "train result: Loss: 1.215130294494629, Acc: 57.694\n",
      "\n",
      "Test: [0/79]\tLoss 1.3779 (1.3779)\tPrec@1 53.906 (53.906)\n",
      "Validation result: Loss: 1.2764678893089294, Acc: 55.34\n",
      "\n",
      "Epoch: [67][0/391]\tLoss 1.1594 (1.1594)\n",
      "Epoch: [67][100/391]\tLoss 1.3128 (1.2087)\n",
      "Epoch: [67][200/391]\tLoss 1.2748 (1.2063)\n",
      "Epoch: [67][300/391]\tLoss 1.4276 (1.2098)\n",
      "train result: Loss: 1.214877721672058, Acc: 57.652\n",
      "\n",
      "Test: [0/79]\tLoss 1.4765 (1.4765)\tPrec@1 47.656 (47.656)\n",
      "Validation result: Loss: 1.2655411399841308, Acc: 56.19\n",
      "\n",
      "Epoch: [68][0/391]\tLoss 1.1492 (1.1492)\n",
      "Epoch: [68][100/391]\tLoss 1.3661 (1.2016)\n",
      "Epoch: [68][200/391]\tLoss 1.2462 (1.2024)\n",
      "Epoch: [68][300/391]\tLoss 1.2214 (1.2080)\n",
      "train result: Loss: 1.2127789628982544, Acc: 57.624\n",
      "\n",
      "Test: [0/79]\tLoss 1.2454 (1.2454)\tPrec@1 61.719 (61.719)\n",
      "Validation result: Loss: 1.2546356893539428, Acc: 56.57\n",
      "\n",
      "Epoch: [69][0/391]\tLoss 0.9833 (0.9833)\n",
      "Epoch: [69][100/391]\tLoss 1.3509 (1.2008)\n",
      "Epoch: [69][200/391]\tLoss 1.0613 (1.1974)\n",
      "Epoch: [69][300/391]\tLoss 1.3769 (1.2025)\n",
      "train result: Loss: 1.2034046212005616, Acc: 57.918\n",
      "\n",
      "Test: [0/79]\tLoss 1.4553 (1.4553)\tPrec@1 46.875 (46.875)\n",
      "Validation result: Loss: 1.2543010340690612, Acc: 56.28\n",
      "\n",
      "Epoch: [70][0/391]\tLoss 1.1828 (1.1828)\n",
      "Epoch: [70][100/391]\tLoss 1.1727 (1.1969)\n",
      "Epoch: [70][200/391]\tLoss 1.3495 (1.2057)\n",
      "Epoch: [70][300/391]\tLoss 1.4297 (1.2095)\n",
      "train result: Loss: 1.2065357298660278, Acc: 57.922\n",
      "\n",
      "Test: [0/79]\tLoss 1.5512 (1.5512)\tPrec@1 50.000 (50.000)\n",
      "Validation result: Loss: 1.284822265148163, Acc: 54.86\n",
      "\n",
      "Epoch: [71][0/391]\tLoss 1.2338 (1.2338)\n",
      "Epoch: [71][100/391]\tLoss 1.1419 (1.1956)\n",
      "Epoch: [71][200/391]\tLoss 1.2907 (1.1902)\n",
      "Epoch: [71][300/391]\tLoss 1.2095 (1.1978)\n",
      "train result: Loss: 1.1997233347320557, Acc: 58.078\n",
      "\n",
      "Test: [0/79]\tLoss 1.4937 (1.4937)\tPrec@1 51.562 (51.562)\n",
      "Validation result: Loss: 1.2556482509613036, Acc: 56.65\n",
      "\n",
      "Epoch: [72][0/391]\tLoss 1.2115 (1.2115)\n",
      "Epoch: [72][100/391]\tLoss 1.1324 (1.1983)\n",
      "Epoch: [72][200/391]\tLoss 1.1793 (1.1952)\n",
      "Epoch: [72][300/391]\tLoss 1.4705 (1.1954)\n",
      "train result: Loss: 1.194168959159851, Acc: 58.288\n",
      "\n",
      "Test: [0/79]\tLoss 1.3380 (1.3380)\tPrec@1 60.938 (60.938)\n",
      "Validation result: Loss: 1.2629484757423401, Acc: 55.97\n",
      "\n",
      "Epoch: [73][0/391]\tLoss 1.1435 (1.1435)\n",
      "Epoch: [73][100/391]\tLoss 1.0793 (1.1732)\n",
      "Epoch: [73][200/391]\tLoss 1.3130 (1.1864)\n",
      "Epoch: [73][300/391]\tLoss 1.0756 (1.1849)\n",
      "train result: Loss: 1.1890366453170775, Acc: 58.63\n",
      "\n",
      "Test: [0/79]\tLoss 1.1472 (1.1472)\tPrec@1 62.500 (62.500)\n",
      "Validation result: Loss: 1.2635683524131776, Acc: 56.12\n",
      "\n",
      "Epoch: [74][0/391]\tLoss 1.2539 (1.2539)\n",
      "Epoch: [74][100/391]\tLoss 1.1088 (1.1775)\n",
      "Epoch: [74][200/391]\tLoss 1.2495 (1.1841)\n",
      "Epoch: [74][300/391]\tLoss 1.1690 (1.1841)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train result: Loss: 1.1843509614181518, Acc: 58.626\n",
      "\n",
      "Test: [0/79]\tLoss 1.3397 (1.3397)\tPrec@1 55.469 (55.469)\n",
      "Validation result: Loss: 1.2678568612098693, Acc: 56.42\n",
      "\n",
      "Epoch: [75][0/391]\tLoss 1.1201 (1.1201)\n",
      "Epoch: [75][100/391]\tLoss 1.1798 (1.1566)\n",
      "Epoch: [75][200/391]\tLoss 1.1377 (1.1730)\n",
      "Epoch: [75][300/391]\tLoss 1.0146 (1.1768)\n",
      "train result: Loss: 1.1776742821884156, Acc: 58.892\n",
      "\n",
      "Test: [0/79]\tLoss 1.3682 (1.3682)\tPrec@1 59.375 (59.375)\n",
      "Validation result: Loss: 1.2460337324142456, Acc: 56.65\n",
      "\n",
      "Epoch: [76][0/391]\tLoss 1.1828 (1.1828)\n",
      "Epoch: [76][100/391]\tLoss 1.1352 (1.1603)\n",
      "Epoch: [76][200/391]\tLoss 1.1791 (1.1687)\n",
      "Epoch: [76][300/391]\tLoss 1.1700 (1.1752)\n",
      "train result: Loss: 1.1757497412872315, Acc: 58.904\n",
      "\n",
      "Test: [0/79]\tLoss 1.2609 (1.2609)\tPrec@1 57.812 (57.812)\n",
      "Validation result: Loss: 1.2313680603027344, Acc: 57.55\n",
      "\n",
      "Epoch: [77][0/391]\tLoss 1.1577 (1.1577)\n",
      "Epoch: [77][100/391]\tLoss 1.2778 (1.1578)\n",
      "Epoch: [77][200/391]\tLoss 1.0762 (1.1646)\n",
      "Epoch: [77][300/391]\tLoss 1.2644 (1.1701)\n",
      "train result: Loss: 1.1725970244979858, Acc: 59.052\n",
      "\n",
      "Test: [0/79]\tLoss 1.2868 (1.2868)\tPrec@1 59.375 (59.375)\n",
      "Validation result: Loss: 1.2622567333221435, Acc: 56.39\n",
      "\n",
      "Epoch: [78][0/391]\tLoss 1.0226 (1.0226)\n",
      "Epoch: [78][100/391]\tLoss 1.0954 (1.1591)\n",
      "Epoch: [78][200/391]\tLoss 1.0763 (1.1643)\n",
      "Epoch: [78][300/391]\tLoss 0.9208 (1.1605)\n",
      "train result: Loss: 1.1626997771835328, Acc: 59.632\n",
      "\n",
      "Test: [0/79]\tLoss 1.2002 (1.2002)\tPrec@1 57.812 (57.812)\n",
      "Validation result: Loss: 1.2430088724136352, Acc: 56.58\n",
      "\n",
      "Epoch: [79][0/391]\tLoss 1.3467 (1.3467)\n",
      "Epoch: [79][100/391]\tLoss 1.1907 (1.1386)\n",
      "Epoch: [79][200/391]\tLoss 1.0645 (1.1499)\n",
      "Epoch: [79][300/391]\tLoss 1.1514 (1.1543)\n",
      "train result: Loss: 1.1593345058441162, Acc: 59.55\n",
      "\n",
      "Test: [0/79]\tLoss 1.3350 (1.3350)\tPrec@1 56.250 (56.250)\n",
      "Validation result: Loss: 1.222823654270172, Acc: 57.55\n",
      "\n",
      "Epoch: [80][0/391]\tLoss 0.9836 (0.9836)\n",
      "Epoch: [80][100/391]\tLoss 1.1527 (1.1327)\n",
      "Epoch: [80][200/391]\tLoss 1.2400 (1.1432)\n",
      "Epoch: [80][300/391]\tLoss 1.2357 (1.1455)\n",
      "train result: Loss: 1.147253886795044, Acc: 60.058\n",
      "\n",
      "Test: [0/79]\tLoss 1.4605 (1.4605)\tPrec@1 53.906 (53.906)\n",
      "Validation result: Loss: 1.2787258053779602, Acc: 55.41\n",
      "\n",
      "Epoch: [81][0/391]\tLoss 1.1567 (1.1567)\n",
      "Epoch: [81][100/391]\tLoss 1.1403 (1.1288)\n",
      "Epoch: [81][200/391]\tLoss 1.3655 (1.1365)\n",
      "Epoch: [81][300/391]\tLoss 0.9662 (1.1414)\n",
      "train result: Loss: 1.1460898258972168, Acc: 60.05\n",
      "\n",
      "Test: [0/79]\tLoss 1.2124 (1.2124)\tPrec@1 57.031 (57.031)\n",
      "Validation result: Loss: 1.1935085260391236, Acc: 58.37\n",
      "\n",
      "Epoch: [82][0/391]\tLoss 1.2442 (1.2442)\n",
      "Epoch: [82][100/391]\tLoss 1.3106 (1.1410)\n",
      "Epoch: [82][200/391]\tLoss 1.1168 (1.1418)\n",
      "Epoch: [82][300/391]\tLoss 1.1631 (1.1447)\n",
      "train result: Loss: 1.1416275650024414, Acc: 60.434\n",
      "\n",
      "Test: [0/79]\tLoss 1.3545 (1.3545)\tPrec@1 60.156 (60.156)\n",
      "Validation result: Loss: 1.229833308982849, Acc: 57.32\n",
      "\n",
      "Epoch: [83][0/391]\tLoss 1.0862 (1.0862)\n",
      "Epoch: [83][100/391]\tLoss 1.1339 (1.1211)\n",
      "Epoch: [83][200/391]\tLoss 1.1687 (1.1353)\n",
      "Epoch: [83][300/391]\tLoss 1.1198 (1.1367)\n",
      "train result: Loss: 1.1370967765808107, Acc: 60.624\n",
      "\n",
      "Test: [0/79]\tLoss 1.3424 (1.3424)\tPrec@1 53.125 (53.125)\n",
      "Validation result: Loss: 1.244653147315979, Acc: 56.51\n",
      "\n",
      "Epoch: [84][0/391]\tLoss 1.1886 (1.1886)\n",
      "Epoch: [84][100/391]\tLoss 1.2919 (1.1267)\n",
      "Epoch: [84][200/391]\tLoss 1.1524 (1.1246)\n",
      "Epoch: [84][300/391]\tLoss 1.1211 (1.1286)\n",
      "train result: Loss: 1.1315968716430664, Acc: 60.604\n",
      "\n",
      "Test: [0/79]\tLoss 1.3688 (1.3688)\tPrec@1 55.469 (55.469)\n",
      "Validation result: Loss: 1.2088096153259278, Acc: 57.69\n",
      "\n",
      "Epoch: [85][0/391]\tLoss 1.2082 (1.2082)\n",
      "Epoch: [85][100/391]\tLoss 1.1659 (1.1158)\n",
      "Epoch: [85][200/391]\tLoss 0.8945 (1.1198)\n",
      "Epoch: [85][300/391]\tLoss 1.0264 (1.1216)\n",
      "train result: Loss: 1.1232089486694337, Acc: 60.718\n",
      "\n",
      "Test: [0/79]\tLoss 1.0945 (1.0945)\tPrec@1 67.188 (67.188)\n",
      "Validation result: Loss: 1.2009021244049072, Acc: 58.34\n",
      "\n",
      "Epoch: [86][0/391]\tLoss 1.0314 (1.0314)\n",
      "Epoch: [86][100/391]\tLoss 1.3201 (1.1191)\n",
      "Epoch: [86][200/391]\tLoss 1.3200 (1.1142)\n",
      "Epoch: [86][300/391]\tLoss 1.0144 (1.1159)\n",
      "train result: Loss: 1.118134658279419, Acc: 60.924\n",
      "\n",
      "Test: [0/79]\tLoss 1.2640 (1.2640)\tPrec@1 60.156 (60.156)\n",
      "Validation result: Loss: 1.2117822818756103, Acc: 57.75\n",
      "\n",
      "Epoch: [87][0/391]\tLoss 1.2744 (1.2744)\n",
      "Epoch: [87][100/391]\tLoss 1.1693 (1.1260)\n",
      "Epoch: [87][200/391]\tLoss 1.2428 (1.1250)\n",
      "Epoch: [87][300/391]\tLoss 1.1073 (1.1229)\n",
      "train result: Loss: 1.12109280128479, Acc: 60.894\n",
      "\n",
      "Test: [0/79]\tLoss 1.4347 (1.4347)\tPrec@1 57.031 (57.031)\n",
      "Validation result: Loss: 1.2219023525238037, Acc: 57.91\n",
      "\n",
      "Epoch: [88][0/391]\tLoss 1.0012 (1.0012)\n",
      "Epoch: [88][100/391]\tLoss 1.0916 (1.1069)\n",
      "Epoch: [88][200/391]\tLoss 1.1038 (1.1101)\n",
      "Epoch: [88][300/391]\tLoss 1.0749 (1.1083)\n",
      "train result: Loss: 1.1100215916633607, Acc: 61.036\n",
      "\n",
      "Test: [0/79]\tLoss 1.3789 (1.3789)\tPrec@1 53.125 (53.125)\n",
      "Validation result: Loss: 1.204579051399231, Acc: 58.25\n",
      "\n",
      "Epoch: [89][0/391]\tLoss 1.3005 (1.3005)\n",
      "Epoch: [89][100/391]\tLoss 0.9599 (1.1015)\n",
      "Epoch: [89][200/391]\tLoss 1.1449 (1.1058)\n",
      "Epoch: [89][300/391]\tLoss 1.1940 (1.1095)\n",
      "train result: Loss: 1.1083541996002197, Acc: 61.236\n",
      "\n",
      "Test: [0/79]\tLoss 1.2138 (1.2138)\tPrec@1 64.062 (64.062)\n",
      "Validation result: Loss: 1.198737866783142, Acc: 58.33\n",
      "\n",
      "Epoch: [90][0/391]\tLoss 1.0210 (1.0210)\n",
      "Epoch: [90][100/391]\tLoss 1.1397 (1.0977)\n",
      "Epoch: [90][200/391]\tLoss 1.0706 (1.0987)\n",
      "Epoch: [90][300/391]\tLoss 1.2618 (1.0999)\n",
      "train result: Loss: 1.1045899092102052, Acc: 61.392\n",
      "\n",
      "Test: [0/79]\tLoss 1.2551 (1.2551)\tPrec@1 60.938 (60.938)\n",
      "Validation result: Loss: 1.1910034999847412, Acc: 58.26\n",
      "\n",
      "Epoch: [91][0/391]\tLoss 1.2395 (1.2395)\n",
      "Epoch: [91][100/391]\tLoss 1.0369 (1.0916)\n",
      "Epoch: [91][200/391]\tLoss 1.1815 (1.1003)\n",
      "Epoch: [91][300/391]\tLoss 0.9752 (1.0999)\n",
      "train result: Loss: 1.0998974585723877, Acc: 61.618\n",
      "\n",
      "Test: [0/79]\tLoss 1.3693 (1.3693)\tPrec@1 53.125 (53.125)\n",
      "Validation result: Loss: 1.207211070919037, Acc: 57.84\n",
      "\n",
      "Epoch: [92][0/391]\tLoss 1.0814 (1.0814)\n",
      "Epoch: [92][100/391]\tLoss 1.2053 (1.1054)\n",
      "Epoch: [92][200/391]\tLoss 1.0722 (1.0998)\n",
      "Epoch: [92][300/391]\tLoss 1.1158 (1.0989)\n",
      "train result: Loss: 1.0983727815437316, Acc: 61.76\n",
      "\n",
      "Test: [0/79]\tLoss 1.3295 (1.3295)\tPrec@1 58.594 (58.594)\n",
      "Validation result: Loss: 1.2095214977264404, Acc: 58.38\n",
      "\n",
      "Epoch: [93][0/391]\tLoss 1.0494 (1.0494)\n",
      "Epoch: [93][100/391]\tLoss 1.0010 (1.0851)\n",
      "Epoch: [93][200/391]\tLoss 1.1350 (1.0852)\n",
      "Epoch: [93][300/391]\tLoss 0.9719 (1.0910)\n",
      "train result: Loss: 1.0945912436294556, Acc: 61.614\n",
      "\n",
      "Test: [0/79]\tLoss 1.3847 (1.3847)\tPrec@1 55.469 (55.469)\n",
      "Validation result: Loss: 1.1980687797546388, Acc: 58.4\n",
      "\n",
      "Epoch: [94][0/391]\tLoss 1.1304 (1.1304)\n",
      "Epoch: [94][100/391]\tLoss 1.0745 (1.0786)\n",
      "Epoch: [94][200/391]\tLoss 1.0583 (1.0893)\n",
      "Epoch: [94][300/391]\tLoss 1.0124 (1.0896)\n",
      "train result: Loss: 1.084549566001892, Acc: 62.298\n",
      "\n",
      "Test: [0/79]\tLoss 1.2756 (1.2756)\tPrec@1 60.156 (60.156)\n",
      "Validation result: Loss: 1.1884128211975098, Acc: 58.93\n",
      "\n",
      "Epoch: [95][0/391]\tLoss 1.0436 (1.0436)\n",
      "Epoch: [95][100/391]\tLoss 1.0984 (1.0679)\n",
      "Epoch: [95][200/391]\tLoss 1.2185 (1.0839)\n",
      "Epoch: [95][300/391]\tLoss 1.0889 (1.0881)\n",
      "train result: Loss: 1.0846518214225769, Acc: 62.094\n",
      "\n",
      "Test: [0/79]\tLoss 1.3207 (1.3207)\tPrec@1 59.375 (59.375)\n",
      "Validation result: Loss: 1.1955255430221559, Acc: 58.6\n",
      "\n",
      "Epoch: [96][0/391]\tLoss 1.0963 (1.0963)\n",
      "Epoch: [96][100/391]\tLoss 1.0327 (1.0688)\n",
      "Epoch: [96][200/391]\tLoss 1.0003 (1.0733)\n",
      "Epoch: [96][300/391]\tLoss 1.1568 (1.0749)\n",
      "train result: Loss: 1.0809180210876466, Acc: 62.286\n",
      "\n",
      "Test: [0/79]\tLoss 1.4788 (1.4788)\tPrec@1 50.781 (50.781)\n",
      "Validation result: Loss: 1.2202425735473632, Acc: 58.42\n",
      "\n",
      "Epoch: [97][0/391]\tLoss 1.0106 (1.0106)\n",
      "Epoch: [97][100/391]\tLoss 1.0184 (1.0754)\n",
      "Epoch: [97][200/391]\tLoss 1.1969 (1.0748)\n",
      "Epoch: [97][300/391]\tLoss 1.1419 (1.0796)\n",
      "train result: Loss: 1.0788611687088012, Acc: 62.272\n",
      "\n",
      "Test: [0/79]\tLoss 1.1836 (1.1836)\tPrec@1 57.812 (57.812)\n",
      "Validation result: Loss: 1.1977005626678467, Acc: 58.92\n",
      "\n",
      "Epoch: [98][0/391]\tLoss 1.0020 (1.0020)\n",
      "Epoch: [98][100/391]\tLoss 1.1030 (1.0484)\n",
      "Epoch: [98][200/391]\tLoss 1.0528 (1.0583)\n",
      "Epoch: [98][300/391]\tLoss 1.0547 (1.0673)\n",
      "train result: Loss: 1.0719688909721374, Acc: 62.748\n",
      "\n",
      "Test: [0/79]\tLoss 1.2520 (1.2520)\tPrec@1 60.938 (60.938)\n",
      "Validation result: Loss: 1.1895743927001954, Acc: 59.14\n",
      "\n",
      "Epoch: [99][0/391]\tLoss 0.8358 (0.8358)\n",
      "Epoch: [99][100/391]\tLoss 1.1954 (1.0650)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [99][200/391]\tLoss 1.0535 (1.0615)\n",
      "Epoch: [99][300/391]\tLoss 1.2057 (1.0644)\n",
      "train result: Loss: 1.0675693277359009, Acc: 62.8\n",
      "\n",
      "Test: [0/79]\tLoss 1.1854 (1.1854)\tPrec@1 60.938 (60.938)\n",
      "Validation result: Loss: 1.1744255101203918, Acc: 59.72\n",
      "\n",
      "Epoch: [100][0/391]\tLoss 0.8956 (0.8956)\n",
      "Epoch: [100][100/391]\tLoss 0.9504 (1.0539)\n",
      "Epoch: [100][200/391]\tLoss 1.0777 (1.0554)\n",
      "Epoch: [100][300/391]\tLoss 1.0878 (1.0628)\n",
      "train result: Loss: 1.0686348652839661, Acc: 62.466\n",
      "\n",
      "Test: [0/79]\tLoss 1.3210 (1.3210)\tPrec@1 57.812 (57.812)\n",
      "Validation result: Loss: 1.1837186338424683, Acc: 58.89\n",
      "\n",
      "Epoch: [101][0/391]\tLoss 0.9639 (0.9639)\n",
      "Epoch: [101][100/391]\tLoss 1.0481 (1.0444)\n",
      "Epoch: [101][200/391]\tLoss 1.1736 (1.0469)\n",
      "Epoch: [101][300/391]\tLoss 1.1435 (1.0578)\n",
      "train result: Loss: 1.0644976950454712, Acc: 62.75\n",
      "\n",
      "Test: [0/79]\tLoss 1.3610 (1.3610)\tPrec@1 54.688 (54.688)\n",
      "Validation result: Loss: 1.2074915249824525, Acc: 58.44\n",
      "\n",
      "Epoch: [102][0/391]\tLoss 1.1495 (1.1495)\n",
      "Epoch: [102][100/391]\tLoss 0.9198 (1.0330)\n",
      "Epoch: [102][200/391]\tLoss 0.9859 (1.0466)\n",
      "Epoch: [102][300/391]\tLoss 0.9422 (1.0470)\n",
      "train result: Loss: 1.0556242439651489, Acc: 63.192\n",
      "\n",
      "Test: [0/79]\tLoss 1.2170 (1.2170)\tPrec@1 64.844 (64.844)\n",
      "Validation result: Loss: 1.195867089176178, Acc: 59.23\n",
      "\n",
      "Epoch: [103][0/391]\tLoss 0.8486 (0.8486)\n",
      "Epoch: [103][100/391]\tLoss 0.9807 (1.0348)\n",
      "Epoch: [103][200/391]\tLoss 0.9767 (1.0427)\n",
      "Epoch: [103][300/391]\tLoss 1.1805 (1.0537)\n",
      "train result: Loss: 1.0525214755630494, Acc: 63.158\n",
      "\n",
      "Test: [0/79]\tLoss 1.2383 (1.2383)\tPrec@1 58.594 (58.594)\n",
      "Validation result: Loss: 1.176800257205963, Acc: 59.82\n",
      "\n",
      "Epoch: [104][0/391]\tLoss 1.0023 (1.0023)\n",
      "Epoch: [104][100/391]\tLoss 1.0862 (1.0323)\n",
      "Epoch: [104][200/391]\tLoss 1.0709 (1.0367)\n",
      "Epoch: [104][300/391]\tLoss 0.9382 (1.0476)\n",
      "train result: Loss: 1.0509662718963624, Acc: 63.362\n",
      "\n",
      "Test: [0/79]\tLoss 1.4890 (1.4890)\tPrec@1 54.688 (54.688)\n",
      "Validation result: Loss: 1.1746498064994813, Acc: 59.55\n",
      "\n",
      "Epoch: [105][0/391]\tLoss 1.1010 (1.1010)\n",
      "Epoch: [105][100/391]\tLoss 1.0481 (1.0291)\n",
      "Epoch: [105][200/391]\tLoss 1.1582 (1.0416)\n",
      "Epoch: [105][300/391]\tLoss 1.0057 (1.0509)\n",
      "train result: Loss: 1.052194446144104, Acc: 63.404\n",
      "\n",
      "Test: [0/79]\tLoss 1.1787 (1.1787)\tPrec@1 61.719 (61.719)\n",
      "Validation result: Loss: 1.1738064931869507, Acc: 59.44\n",
      "\n",
      "Epoch: [106][0/391]\tLoss 1.0097 (1.0097)\n",
      "Epoch: [106][100/391]\tLoss 1.0722 (1.0475)\n",
      "Epoch: [106][200/391]\tLoss 0.8897 (1.0474)\n",
      "Epoch: [106][300/391]\tLoss 1.0902 (1.0459)\n",
      "train result: Loss: 1.0469300246047974, Acc: 63.496\n",
      "\n",
      "Test: [0/79]\tLoss 1.4830 (1.4830)\tPrec@1 54.688 (54.688)\n",
      "Validation result: Loss: 1.1968024656295777, Acc: 58.38\n",
      "\n",
      "Epoch: [107][0/391]\tLoss 1.1365 (1.1365)\n",
      "Epoch: [107][100/391]\tLoss 0.9851 (1.0449)\n",
      "Epoch: [107][200/391]\tLoss 1.1311 (1.0452)\n",
      "Epoch: [107][300/391]\tLoss 0.8171 (1.0464)\n",
      "train result: Loss: 1.0439226544570923, Acc: 63.522\n",
      "\n",
      "Test: [0/79]\tLoss 1.2906 (1.2906)\tPrec@1 64.062 (64.062)\n",
      "Validation result: Loss: 1.1791610090255737, Acc: 59.45\n",
      "\n",
      "Epoch: [108][0/391]\tLoss 0.9958 (0.9958)\n",
      "Epoch: [108][100/391]\tLoss 1.1691 (1.0166)\n",
      "Epoch: [108][200/391]\tLoss 1.0172 (1.0253)\n",
      "Epoch: [108][300/391]\tLoss 1.0677 (1.0333)\n",
      "train result: Loss: 1.03922347530365, Acc: 63.78\n",
      "\n",
      "Test: [0/79]\tLoss 1.2089 (1.2089)\tPrec@1 58.594 (58.594)\n",
      "Validation result: Loss: 1.1910967721939087, Acc: 58.98\n",
      "\n",
      "Epoch: [109][0/391]\tLoss 0.9763 (0.9763)\n",
      "Epoch: [109][100/391]\tLoss 1.0591 (1.0350)\n",
      "Epoch: [109][200/391]\tLoss 1.1198 (1.0357)\n",
      "Epoch: [109][300/391]\tLoss 0.9804 (1.0391)\n",
      "train result: Loss: 1.0417724659347534, Acc: 63.598\n",
      "\n",
      "Test: [0/79]\tLoss 1.2173 (1.2173)\tPrec@1 61.719 (61.719)\n",
      "Validation result: Loss: 1.2136589231491088, Acc: 58.69\n",
      "\n",
      "Epoch: [110][0/391]\tLoss 1.0173 (1.0173)\n",
      "Epoch: [110][100/391]\tLoss 1.0713 (1.0134)\n",
      "Epoch: [110][200/391]\tLoss 0.9962 (1.0213)\n",
      "Epoch: [110][300/391]\tLoss 0.9567 (1.0270)\n",
      "train result: Loss: 1.0303602773666383, Acc: 64.072\n",
      "\n",
      "Test: [0/79]\tLoss 1.2805 (1.2805)\tPrec@1 57.031 (57.031)\n",
      "Validation result: Loss: 1.2065925277709961, Acc: 58.48\n",
      "\n",
      "Epoch: [111][0/391]\tLoss 0.9595 (0.9595)\n",
      "Epoch: [111][100/391]\tLoss 0.8778 (1.0167)\n",
      "Epoch: [111][200/391]\tLoss 0.8968 (1.0252)\n",
      "Epoch: [111][300/391]\tLoss 1.0032 (1.0298)\n",
      "train result: Loss: 1.0294669482803345, Acc: 63.98\n",
      "\n",
      "Test: [0/79]\tLoss 1.0909 (1.0909)\tPrec@1 62.500 (62.500)\n",
      "Validation result: Loss: 1.1982502447128296, Acc: 59.03\n",
      "\n",
      "Epoch: [112][0/391]\tLoss 0.9348 (0.9348)\n",
      "Epoch: [112][100/391]\tLoss 0.9704 (1.0000)\n",
      "Epoch: [112][200/391]\tLoss 0.9441 (1.0142)\n",
      "Epoch: [112][300/391]\tLoss 1.0531 (1.0237)\n",
      "train result: Loss: 1.029443773765564, Acc: 64.132\n",
      "\n",
      "Test: [0/79]\tLoss 1.1874 (1.1874)\tPrec@1 57.031 (57.031)\n",
      "Validation result: Loss: 1.1743863666534424, Acc: 59.15\n",
      "\n",
      "Epoch: [113][0/391]\tLoss 0.8872 (0.8872)\n",
      "Epoch: [113][100/391]\tLoss 1.1601 (1.0319)\n",
      "Epoch: [113][200/391]\tLoss 1.1897 (1.0209)\n",
      "Epoch: [113][300/391]\tLoss 0.9114 (1.0183)\n",
      "train result: Loss: 1.0215996490859984, Acc: 64.342\n",
      "\n",
      "Test: [0/79]\tLoss 0.8405 (0.8405)\tPrec@1 71.875 (71.875)\n",
      "Validation result: Loss: 1.1812214790344238, Acc: 59.56\n",
      "\n",
      "Epoch: [114][0/391]\tLoss 0.9842 (0.9842)\n",
      "Epoch: [114][100/391]\tLoss 1.0749 (1.0046)\n",
      "Epoch: [114][200/391]\tLoss 1.1290 (1.0107)\n",
      "Epoch: [114][300/391]\tLoss 1.0936 (1.0161)\n",
      "train result: Loss: 1.0218333777046205, Acc: 64.252\n",
      "\n",
      "Test: [0/79]\tLoss 1.1291 (1.1291)\tPrec@1 66.406 (66.406)\n",
      "Validation result: Loss: 1.1808906461715698, Acc: 58.87\n",
      "\n",
      "Epoch: [115][0/391]\tLoss 1.0101 (1.0101)\n",
      "Epoch: [115][100/391]\tLoss 0.9436 (0.9923)\n",
      "Epoch: [115][200/391]\tLoss 0.9320 (0.9977)\n",
      "Epoch: [115][300/391]\tLoss 0.8875 (1.0054)\n",
      "train result: Loss: 1.0149374047088624, Acc: 64.482\n",
      "\n",
      "Test: [0/79]\tLoss 0.9973 (0.9973)\tPrec@1 68.750 (68.750)\n",
      "Validation result: Loss: 1.1833082433700561, Acc: 59.24\n",
      "\n",
      "Epoch: [116][0/391]\tLoss 0.8244 (0.8244)\n",
      "Epoch: [116][100/391]\tLoss 0.9382 (0.9929)\n",
      "Epoch: [116][200/391]\tLoss 0.9818 (0.9988)\n",
      "Epoch: [116][300/391]\tLoss 1.0195 (0.9993)\n",
      "train result: Loss: 1.006992579460144, Acc: 64.868\n",
      "\n",
      "Test: [0/79]\tLoss 1.4525 (1.4525)\tPrec@1 49.219 (49.219)\n",
      "Validation result: Loss: 1.199157050037384, Acc: 58.75\n",
      "\n",
      "Epoch: [117][0/391]\tLoss 1.0025 (1.0025)\n",
      "Epoch: [117][100/391]\tLoss 1.0741 (0.9998)\n",
      "Epoch: [117][200/391]\tLoss 0.9925 (1.0029)\n",
      "Epoch: [117][300/391]\tLoss 0.9352 (1.0083)\n",
      "train result: Loss: 1.0096333030319213, Acc: 64.824\n",
      "\n",
      "Test: [0/79]\tLoss 0.9549 (0.9549)\tPrec@1 67.969 (67.969)\n",
      "Validation result: Loss: 1.1822161929130555, Acc: 59.26\n",
      "\n",
      "Epoch: [118][0/391]\tLoss 0.9827 (0.9827)\n",
      "Epoch: [118][100/391]\tLoss 1.0046 (1.0027)\n",
      "Epoch: [118][200/391]\tLoss 0.9533 (1.0043)\n",
      "Epoch: [118][300/391]\tLoss 1.0133 (1.0060)\n",
      "train result: Loss: 1.0067529365730286, Acc: 64.88\n",
      "\n",
      "Test: [0/79]\tLoss 1.0843 (1.0843)\tPrec@1 67.969 (67.969)\n",
      "Validation result: Loss: 1.163566381931305, Acc: 59.87\n",
      "\n",
      "Epoch: [119][0/391]\tLoss 1.0506 (1.0506)\n",
      "Epoch: [119][100/391]\tLoss 0.8983 (0.9969)\n",
      "Epoch: [119][200/391]\tLoss 0.9003 (1.0001)\n",
      "Epoch: [119][300/391]\tLoss 1.0670 (1.0033)\n",
      "train result: Loss: 1.004332215499878, Acc: 64.766\n",
      "\n",
      "Test: [0/79]\tLoss 1.2425 (1.2425)\tPrec@1 57.812 (57.812)\n",
      "Validation result: Loss: 1.1559261123657227, Acc: 60.32\n",
      "\n",
      "Epoch: [120][0/391]\tLoss 1.1440 (1.1440)\n",
      "Epoch: [120][100/391]\tLoss 1.0141 (0.9955)\n",
      "Epoch: [120][200/391]\tLoss 0.9111 (1.0045)\n",
      "Epoch: [120][300/391]\tLoss 0.9557 (1.0017)\n",
      "train result: Loss: 1.0023552116775514, Acc: 64.858\n",
      "\n",
      "Test: [0/79]\tLoss 1.4196 (1.4196)\tPrec@1 54.688 (54.688)\n",
      "Validation result: Loss: 1.1740633875846862, Acc: 59.75\n",
      "\n",
      "Epoch: [121][0/391]\tLoss 0.9491 (0.9491)\n",
      "Epoch: [121][100/391]\tLoss 0.9658 (0.9874)\n",
      "Epoch: [121][200/391]\tLoss 0.9841 (0.9955)\n",
      "Epoch: [121][300/391]\tLoss 1.1148 (0.9977)\n",
      "train result: Loss: 1.0036784162902832, Acc: 64.736\n",
      "\n",
      "Test: [0/79]\tLoss 1.1748 (1.1748)\tPrec@1 64.062 (64.062)\n",
      "Validation result: Loss: 1.1556887063980104, Acc: 60.12\n",
      "\n",
      "Epoch: [122][0/391]\tLoss 0.9882 (0.9882)\n",
      "Epoch: [122][100/391]\tLoss 1.0997 (0.9678)\n",
      "Epoch: [122][200/391]\tLoss 0.9206 (0.9807)\n",
      "Epoch: [122][300/391]\tLoss 0.9736 (0.9877)\n",
      "train result: Loss: 0.9885057222366334, Acc: 65.488\n",
      "\n",
      "Test: [0/79]\tLoss 0.9509 (0.9509)\tPrec@1 67.188 (67.188)\n",
      "Validation result: Loss: 1.193098539352417, Acc: 59.75\n",
      "\n",
      "Epoch: [123][0/391]\tLoss 0.9655 (0.9655)\n",
      "Epoch: [123][100/391]\tLoss 1.1059 (0.9851)\n",
      "Epoch: [123][200/391]\tLoss 1.0841 (0.9837)\n",
      "Epoch: [123][300/391]\tLoss 0.9546 (0.9872)\n",
      "train result: Loss: 0.9952524122428894, Acc: 65.424\n",
      "\n",
      "Test: [0/79]\tLoss 1.1939 (1.1939)\tPrec@1 64.844 (64.844)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation result: Loss: 1.1986898675918578, Acc: 59.17\n",
      "\n",
      "Epoch: [124][0/391]\tLoss 0.9959 (0.9959)\n",
      "Epoch: [124][100/391]\tLoss 0.8599 (0.9643)\n",
      "Epoch: [124][200/391]\tLoss 0.8092 (0.9860)\n",
      "Epoch: [124][300/391]\tLoss 1.0671 (0.9848)\n",
      "train result: Loss: 0.9904751871109009, Acc: 65.382\n",
      "\n",
      "Test: [0/79]\tLoss 1.3565 (1.3565)\tPrec@1 53.906 (53.906)\n",
      "Validation result: Loss: 1.186376131439209, Acc: 59.19\n",
      "\n",
      "Epoch: [125][0/391]\tLoss 0.9250 (0.9250)\n",
      "Epoch: [125][100/391]\tLoss 0.9591 (0.9860)\n",
      "Epoch: [125][200/391]\tLoss 1.0017 (0.9818)\n",
      "Epoch: [125][300/391]\tLoss 0.9364 (0.9824)\n",
      "train result: Loss: 0.9850133124160767, Acc: 65.764\n",
      "\n",
      "Test: [0/79]\tLoss 1.3177 (1.3177)\tPrec@1 59.375 (59.375)\n",
      "Validation result: Loss: 1.1897947701454163, Acc: 59.8\n",
      "\n",
      "Epoch: [126][0/391]\tLoss 0.9018 (0.9018)\n",
      "Epoch: [126][100/391]\tLoss 0.9962 (0.9744)\n",
      "Epoch: [126][200/391]\tLoss 1.0844 (0.9755)\n",
      "Epoch: [126][300/391]\tLoss 1.0021 (0.9800)\n",
      "train result: Loss: 0.9849988412094116, Acc: 65.658\n",
      "\n",
      "Test: [0/79]\tLoss 1.1192 (1.1192)\tPrec@1 66.406 (66.406)\n",
      "Validation result: Loss: 1.1853551624298095, Acc: 59.44\n",
      "\n",
      "Epoch: [127][0/391]\tLoss 0.7821 (0.7821)\n",
      "Epoch: [127][100/391]\tLoss 0.9918 (0.9619)\n",
      "Epoch: [127][200/391]\tLoss 0.9061 (0.9718)\n",
      "Epoch: [127][300/391]\tLoss 1.0217 (0.9797)\n",
      "train result: Loss: 0.9812002640914917, Acc: 65.646\n",
      "\n",
      "Test: [0/79]\tLoss 1.3037 (1.3037)\tPrec@1 57.031 (57.031)\n",
      "Validation result: Loss: 1.166278634262085, Acc: 60.4\n",
      "\n",
      "Epoch: [128][0/391]\tLoss 0.9356 (0.9356)\n",
      "Epoch: [128][100/391]\tLoss 0.9468 (0.9603)\n",
      "Epoch: [128][200/391]\tLoss 1.0895 (0.9717)\n",
      "Epoch: [128][300/391]\tLoss 1.1405 (0.9763)\n",
      "train result: Loss: 0.9809997751045227, Acc: 65.608\n",
      "\n",
      "Test: [0/79]\tLoss 1.2760 (1.2760)\tPrec@1 61.719 (61.719)\n",
      "Validation result: Loss: 1.17188399143219, Acc: 59.96\n",
      "\n",
      "Epoch: [129][0/391]\tLoss 1.0020 (1.0020)\n",
      "Epoch: [129][100/391]\tLoss 0.8739 (0.9625)\n",
      "Epoch: [129][200/391]\tLoss 1.0566 (0.9633)\n",
      "Epoch: [129][300/391]\tLoss 0.8537 (0.9731)\n",
      "train result: Loss: 0.9770827047729492, Acc: 65.804\n",
      "\n",
      "Test: [0/79]\tLoss 1.0195 (1.0195)\tPrec@1 67.969 (67.969)\n",
      "Validation result: Loss: 1.1853286157608032, Acc: 59.95\n",
      "\n",
      "Epoch: [130][0/391]\tLoss 1.1040 (1.1040)\n",
      "Epoch: [130][100/391]\tLoss 1.0338 (0.9479)\n",
      "Epoch: [130][200/391]\tLoss 0.9615 (0.9670)\n",
      "Epoch: [130][300/391]\tLoss 1.0088 (0.9688)\n",
      "train result: Loss: 0.9703398812675476, Acc: 66.096\n",
      "\n",
      "Test: [0/79]\tLoss 1.2793 (1.2793)\tPrec@1 59.375 (59.375)\n",
      "Validation result: Loss: 1.2107147694587708, Acc: 58.98\n",
      "\n",
      "Epoch: [131][0/391]\tLoss 0.9101 (0.9101)\n",
      "Epoch: [131][100/391]\tLoss 0.9084 (0.9498)\n",
      "Epoch: [131][200/391]\tLoss 0.9784 (0.9601)\n",
      "Epoch: [131][300/391]\tLoss 0.9388 (0.9567)\n",
      "train result: Loss: 0.9611693209648132, Acc: 66.316\n",
      "\n",
      "Test: [0/79]\tLoss 1.2121 (1.2121)\tPrec@1 60.938 (60.938)\n",
      "Validation result: Loss: 1.1929214707374574, Acc: 60.03\n",
      "\n",
      "Epoch: [132][0/391]\tLoss 0.9238 (0.9238)\n",
      "Epoch: [132][100/391]\tLoss 1.0075 (0.9486)\n",
      "Epoch: [132][200/391]\tLoss 0.9180 (0.9547)\n",
      "Epoch: [132][300/391]\tLoss 0.8441 (0.9597)\n",
      "train result: Loss: 0.9634792775344848, Acc: 66.126\n",
      "\n",
      "Test: [0/79]\tLoss 1.1789 (1.1789)\tPrec@1 63.281 (63.281)\n",
      "Validation result: Loss: 1.1824183387756348, Acc: 60.02\n",
      "\n",
      "Epoch: [133][0/391]\tLoss 1.0101 (1.0101)\n",
      "Epoch: [133][100/391]\tLoss 1.0063 (0.9492)\n",
      "Epoch: [133][200/391]\tLoss 0.9419 (0.9549)\n",
      "Epoch: [133][300/391]\tLoss 0.9584 (0.9634)\n",
      "train result: Loss: 0.9658379286193848, Acc: 66.192\n",
      "\n",
      "Test: [0/79]\tLoss 1.8305 (1.8305)\tPrec@1 46.094 (46.094)\n",
      "Validation result: Loss: 1.2218432762145996, Acc: 58.94\n",
      "\n",
      "Epoch: [134][0/391]\tLoss 0.9415 (0.9415)\n",
      "Epoch: [134][100/391]\tLoss 0.9294 (0.9693)\n",
      "Epoch: [134][200/391]\tLoss 0.9495 (0.9602)\n",
      "Epoch: [134][300/391]\tLoss 0.8979 (0.9577)\n",
      "train result: Loss: 0.959182585887909, Acc: 66.462\n",
      "\n",
      "Test: [0/79]\tLoss 1.2194 (1.2194)\tPrec@1 61.719 (61.719)\n",
      "Validation result: Loss: 1.2030477320671082, Acc: 59.55\n",
      "\n",
      "Epoch: [135][0/391]\tLoss 0.8171 (0.8171)\n",
      "Epoch: [135][100/391]\tLoss 0.8882 (0.9355)\n",
      "Epoch: [135][200/391]\tLoss 0.8738 (0.9510)\n",
      "Epoch: [135][300/391]\tLoss 1.0853 (0.9564)\n",
      "train result: Loss: 0.9573002063369751, Acc: 66.706\n",
      "\n",
      "Test: [0/79]\tLoss 1.2526 (1.2526)\tPrec@1 56.250 (56.250)\n",
      "Validation result: Loss: 1.2066217506408692, Acc: 59.33\n",
      "\n",
      "Epoch: [136][0/391]\tLoss 0.9188 (0.9188)\n",
      "Epoch: [136][100/391]\tLoss 1.1052 (0.9465)\n",
      "Epoch: [136][200/391]\tLoss 0.8880 (0.9476)\n",
      "Epoch: [136][300/391]\tLoss 1.0844 (0.9489)\n",
      "train result: Loss: 0.9511047924613952, Acc: 66.712\n",
      "\n",
      "Test: [0/79]\tLoss 1.3782 (1.3782)\tPrec@1 56.250 (56.250)\n",
      "Validation result: Loss: 1.2010659383773803, Acc: 59.97\n",
      "\n",
      "Epoch: [137][0/391]\tLoss 0.8691 (0.8691)\n",
      "Epoch: [137][100/391]\tLoss 0.9612 (0.9419)\n",
      "Epoch: [137][200/391]\tLoss 0.9286 (0.9498)\n",
      "Epoch: [137][300/391]\tLoss 0.8635 (0.9491)\n",
      "train result: Loss: 0.948877889842987, Acc: 66.858\n",
      "\n",
      "Test: [0/79]\tLoss 1.2139 (1.2139)\tPrec@1 60.938 (60.938)\n",
      "Validation result: Loss: 1.1930641830444335, Acc: 59.8\n",
      "\n",
      "Epoch: [138][0/391]\tLoss 1.0533 (1.0533)\n",
      "Epoch: [138][100/391]\tLoss 0.6979 (0.9240)\n",
      "Epoch: [138][200/391]\tLoss 1.0392 (0.9315)\n",
      "Epoch: [138][300/391]\tLoss 0.9208 (0.9416)\n",
      "train result: Loss: 0.9421519859695434, Acc: 67.046\n",
      "\n",
      "Test: [0/79]\tLoss 1.1928 (1.1928)\tPrec@1 62.500 (62.500)\n",
      "Validation result: Loss: 1.1741953213691712, Acc: 60.1\n",
      "\n",
      "Epoch: [139][0/391]\tLoss 0.8780 (0.8780)\n",
      "Epoch: [139][100/391]\tLoss 1.0943 (0.9464)\n",
      "Epoch: [139][200/391]\tLoss 0.8371 (0.9465)\n",
      "Epoch: [139][300/391]\tLoss 0.9370 (0.9422)\n",
      "train result: Loss: 0.9420611846542358, Acc: 67.104\n",
      "\n",
      "Test: [0/79]\tLoss 1.0749 (1.0749)\tPrec@1 69.531 (69.531)\n",
      "Validation result: Loss: 1.19731987285614, Acc: 60.49\n",
      "\n",
      "Epoch: [140][0/391]\tLoss 1.0063 (1.0063)\n",
      "Epoch: [140][100/391]\tLoss 0.8372 (0.9177)\n",
      "Epoch: [140][200/391]\tLoss 1.0684 (0.9296)\n",
      "Epoch: [140][300/391]\tLoss 0.8529 (0.9291)\n",
      "train result: Loss: 0.9337033848190308, Acc: 67.368\n",
      "\n",
      "Test: [0/79]\tLoss 1.2265 (1.2265)\tPrec@1 57.031 (57.031)\n",
      "Validation result: Loss: 1.1874186825752258, Acc: 59.96\n",
      "\n",
      "Epoch: [141][0/391]\tLoss 0.9195 (0.9195)\n",
      "Epoch: [141][100/391]\tLoss 0.9812 (0.9255)\n",
      "Epoch: [141][200/391]\tLoss 0.9285 (0.9270)\n",
      "Epoch: [141][300/391]\tLoss 0.9393 (0.9359)\n",
      "train result: Loss: 0.9389607204437256, Acc: 67.398\n",
      "\n",
      "Test: [0/79]\tLoss 1.1780 (1.1780)\tPrec@1 65.625 (65.625)\n",
      "Validation result: Loss: 1.2015980093955994, Acc: 59.72\n",
      "\n",
      "Epoch: [142][0/391]\tLoss 0.8625 (0.8625)\n",
      "Epoch: [142][100/391]\tLoss 0.7724 (0.9129)\n",
      "Epoch: [142][200/391]\tLoss 0.7311 (0.9220)\n",
      "Epoch: [142][300/391]\tLoss 0.9639 (0.9283)\n",
      "train result: Loss: 0.9321933847236633, Acc: 67.566\n",
      "\n",
      "Test: [0/79]\tLoss 0.9354 (0.9354)\tPrec@1 67.969 (67.969)\n",
      "Validation result: Loss: 1.1698931918621063, Acc: 60.44\n",
      "\n",
      "Epoch: [143][0/391]\tLoss 0.8103 (0.8103)\n",
      "Epoch: [143][100/391]\tLoss 0.9341 (0.8886)\n",
      "Epoch: [143][200/391]\tLoss 0.7799 (0.9117)\n",
      "Epoch: [143][300/391]\tLoss 0.9221 (0.9226)\n",
      "train result: Loss: 0.9237310505676269, Acc: 67.786\n",
      "\n",
      "Test: [0/79]\tLoss 1.0380 (1.0380)\tPrec@1 63.281 (63.281)\n",
      "Validation result: Loss: 1.2326411529541015, Acc: 59.64\n",
      "\n",
      "Epoch: [144][0/391]\tLoss 0.8184 (0.8184)\n",
      "Epoch: [144][100/391]\tLoss 1.0091 (0.9042)\n",
      "Epoch: [144][200/391]\tLoss 0.8965 (0.9136)\n",
      "Epoch: [144][300/391]\tLoss 0.9769 (0.9224)\n",
      "train result: Loss: 0.9267813631439209, Acc: 67.726\n",
      "\n",
      "Test: [0/79]\tLoss 1.4873 (1.4873)\tPrec@1 57.031 (57.031)\n",
      "Validation result: Loss: 1.1840486706733704, Acc: 60.18\n",
      "\n",
      "Epoch: [145][0/391]\tLoss 0.8755 (0.8755)\n",
      "Epoch: [145][100/391]\tLoss 0.9647 (0.9045)\n",
      "Epoch: [145][200/391]\tLoss 1.0346 (0.9162)\n",
      "Epoch: [145][300/391]\tLoss 0.8196 (0.9155)\n",
      "train result: Loss: 0.9228853605270386, Acc: 67.706\n",
      "\n",
      "Test: [0/79]\tLoss 1.3964 (1.3964)\tPrec@1 57.812 (57.812)\n",
      "Validation result: Loss: 1.1728817779541016, Acc: 60.44\n",
      "\n",
      "Epoch: [146][0/391]\tLoss 0.9320 (0.9320)\n",
      "Epoch: [146][100/391]\tLoss 0.9529 (0.9077)\n",
      "Epoch: [146][200/391]\tLoss 1.0026 (0.9158)\n",
      "Epoch: [146][300/391]\tLoss 0.7985 (0.9244)\n",
      "train result: Loss: 0.9287273817253113, Acc: 67.6\n",
      "\n",
      "Test: [0/79]\tLoss 1.2885 (1.2885)\tPrec@1 59.375 (59.375)\n",
      "Validation result: Loss: 1.2127123293876647, Acc: 59.7\n",
      "\n",
      "Epoch: [147][0/391]\tLoss 0.9191 (0.9191)\n",
      "Epoch: [147][100/391]\tLoss 0.9829 (0.9054)\n",
      "Epoch: [147][200/391]\tLoss 0.7839 (0.9120)\n",
      "Epoch: [147][300/391]\tLoss 1.1342 (0.9113)\n",
      "train result: Loss: 0.9184466457748414, Acc: 67.906\n",
      "\n",
      "Test: [0/79]\tLoss 1.3041 (1.3041)\tPrec@1 58.594 (58.594)\n",
      "Validation result: Loss: 1.18097312002182, Acc: 60.33\n",
      "\n",
      "Epoch: [148][0/391]\tLoss 0.8800 (0.8800)\n",
      "Epoch: [148][100/391]\tLoss 0.8902 (0.9025)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [148][200/391]\tLoss 0.8239 (0.8989)\n",
      "Epoch: [148][300/391]\tLoss 0.7513 (0.9084)\n",
      "train result: Loss: 0.9139231258964539, Acc: 68.346\n",
      "\n",
      "Test: [0/79]\tLoss 1.1607 (1.1607)\tPrec@1 64.062 (64.062)\n",
      "Validation result: Loss: 1.188513431930542, Acc: 60.58\n",
      "\n",
      "Epoch: [149][0/391]\tLoss 0.8815 (0.8815)\n",
      "Epoch: [149][100/391]\tLoss 0.8596 (0.8954)\n",
      "Epoch: [149][200/391]\tLoss 0.9488 (0.9008)\n",
      "Epoch: [149][300/391]\tLoss 0.7790 (0.9097)\n",
      "train result: Loss: 0.9125127557182312, Acc: 67.85\n",
      "\n",
      "Test: [0/79]\tLoss 1.2314 (1.2314)\tPrec@1 61.719 (61.719)\n",
      "Validation result: Loss: 1.1888699719429017, Acc: 59.91\n",
      "\n"
     ]
    }
   ],
   "source": [
    "GPU_NUM = 0\n",
    "lr = 1e-2\n",
    "batch_size = 128\n",
    "epochs = 150\n",
    "num_classes = 10\n",
    "\n",
    "device = torch.device(f'cuda:{GPU_NUM}' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "train_dataset = dataset(is_train=True)\n",
    "val_dataset = dataset(is_train=False)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=False)\n",
    "\n",
    "\n",
    "model = SimpleFCN(num_classes=num_classes).to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "train_loss_arr = []\n",
    "train_acc_arr = []\n",
    "\n",
    "val_loss_arr = []\n",
    "val_acc_arr = []\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    for i, (data, target) in enumerate(train_loader):\n",
    "        data = data.reshape(-1, height*width*ch)\n",
    "        \n",
    "        data = data.to(device)\n",
    "        \n",
    "        target = target.to(device)\n",
    "\n",
    "        output = model(data) \n",
    "\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        output.float()\n",
    "        loss.float()\n",
    "\n",
    "        prec1 = accuracy(output.data, target)\n",
    "        prec1 = prec1[0]\n",
    "\n",
    "        losses.update(loss.item(), data.size(0))\n",
    "        top1.update(prec1.item(), data.size(0))\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})'.format(epoch, i, len(train_loader), loss=losses))\n",
    "\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss_arr.append(losses.avg)\n",
    "    train_acc_arr.append(top1.avg)\n",
    "    print(\"train result: Loss: {}, Acc: {}\\n\".format(losses.avg, top1.avg))\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss_sum = 0\n",
    "        val_acc_sum = 0\n",
    "\n",
    "        losses = AverageMeter()\n",
    "        top1 = AverageMeter()\n",
    "\n",
    "        for i, (data, target) in enumerate(val_loader):\n",
    "            data = data.reshape(-1, height*width*ch)\n",
    "\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            output = model(data) \n",
    "\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            output.float()\n",
    "            loss.float()\n",
    "\n",
    "            prec1 = accuracy(output.data, target)\n",
    "\n",
    "            prec1 = prec1[0]\n",
    "            \n",
    "            losses.update(loss.item(), data.size(0))\n",
    "            top1.update(prec1.item(), data.size(0))\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print('Test: [{0}/{1}]\\t'\n",
    "                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                      'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
    "                          i, len(val_loader), loss=losses, top1=top1))\n",
    "\n",
    "        val_loss_arr.append(losses.avg)\n",
    "        val_acc_arr.append(top1.avg)\n",
    "        print(\"Validation result: Loss: {}, Acc: {}\\n\".format(losses.avg, top1.avg))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max(train_acc) 68.346 max(val_acc) 60.58)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3. (Build CNN model with fc layer)\n",
    "- step 1. Build your simple CNN layer by using convolutional layer and fully connected layer\n",
    "- step 2. train your own model\n",
    "- Do not use coding lecture sample model connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# height, width, ch = 32, 32, 3\n",
    "\n",
    "class CNN_model(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN_model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=ch, out_channels=10, \n",
    "                               kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        # 32x32x3 -> 32x32x10 \n",
    "        self.conv2 = nn.Conv2d(in_channels=10, out_channels=20,\n",
    "                               kernel_size=3, stride=1, padding=0, bias=False)\n",
    "        # 32x32x10 -> 30x30x20\n",
    "        self.conv3 = nn.Conv2d(in_channels=20, out_channels=30,\n",
    "                               kernel_size=3, stride=1, padding=0, bias=False)\n",
    "        # 30x30x20 -> 28x28x30\n",
    "        self.conv4 = nn.Conv2d(in_channels=30, out_channels=40,\n",
    "                               kernel_size=3, stride=1, padding=0, bias=False)\n",
    "        # 28x28x30 -> 26x26x40\n",
    "        self.conv5 = nn.Conv2d(in_channels=40, out_channels=50,\n",
    "                               kernel_size=3, stride=1, padding=0, bias=False)\n",
    "        # 26x26x40 -> [24x24x50]\n",
    "        self.fc1 = nn.Linear(24 * 24 * 50, 2048)\n",
    "        self.fc2 = nn.Linear(2048, 2048)\n",
    "        self.fc3 = nn.Linear(2048, 1024)\n",
    "        self.fc4 = nn.Linear(1024, num_classes)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = x.view(-1, 24 * 24 * 50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        output = self.fc4(x)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][0/391]\tLoss 2.3041 (2.3041)\n",
      "Epoch: [0][100/391]\tLoss 2.3028 (2.3028)\n",
      "Epoch: [0][200/391]\tLoss 2.3023 (2.3028)\n",
      "Epoch: [0][300/391]\tLoss 2.3023 (2.3028)\n",
      "train result: Loss: 2.302725789337158, Acc: 9.98\n",
      "\n",
      "Test: [0/79]\tLoss 2.2952 (2.2952)\tPrec@1 0.000 (0.000)\n",
      "Validation result: Loss: 2.3021805423736574, Acc: 10.0\n",
      "\n",
      "Epoch: [1][0/391]\tLoss 2.3000 (2.3000)\n",
      "Epoch: [1][100/391]\tLoss 2.2989 (2.3020)\n",
      "Epoch: [1][200/391]\tLoss 2.2990 (2.3015)\n",
      "Epoch: [1][300/391]\tLoss 2.2625 (2.2946)\n",
      "train result: Loss: 2.261163307647705, Acc: 14.632\n",
      "\n",
      "Test: [0/79]\tLoss 1.6624 (1.6624)\tPrec@1 45.312 (45.312)\n",
      "Validation result: Loss: 2.0580987791061403, Acc: 24.93\n",
      "\n",
      "Epoch: [2][0/391]\tLoss 2.0915 (2.0915)\n",
      "Epoch: [2][100/391]\tLoss 1.9925 (2.0465)\n",
      "Epoch: [2][200/391]\tLoss 1.9271 (2.0249)\n",
      "Epoch: [2][300/391]\tLoss 1.9192 (1.9954)\n",
      "train result: Loss: 1.9676508856201171, Acc: 27.926\n",
      "\n",
      "Test: [0/79]\tLoss 1.6390 (1.6390)\tPrec@1 32.031 (32.031)\n",
      "Validation result: Loss: 1.7970920721054078, Acc: 36.62\n",
      "\n",
      "Epoch: [3][0/391]\tLoss 1.7887 (1.7887)\n",
      "Epoch: [3][100/391]\tLoss 1.7179 (1.8297)\n",
      "Epoch: [3][200/391]\tLoss 1.7213 (1.8114)\n",
      "Epoch: [3][300/391]\tLoss 1.6886 (1.7919)\n",
      "train result: Loss: 1.7756305892181397, Acc: 35.326\n",
      "\n",
      "Test: [0/79]\tLoss 1.8267 (1.8267)\tPrec@1 35.156 (35.156)\n",
      "Validation result: Loss: 1.6064549142837525, Acc: 41.78\n",
      "\n",
      "Epoch: [4][0/391]\tLoss 1.5850 (1.5850)\n",
      "Epoch: [4][100/391]\tLoss 1.6978 (1.6948)\n",
      "Epoch: [4][200/391]\tLoss 1.5834 (1.6880)\n",
      "Epoch: [4][300/391]\tLoss 1.6448 (1.6706)\n",
      "train result: Loss: 1.6645256970977784, Acc: 38.848\n",
      "\n",
      "Test: [0/79]\tLoss 1.4727 (1.4727)\tPrec@1 40.625 (40.625)\n",
      "Validation result: Loss: 1.4846525032043456, Acc: 45.28\n",
      "\n",
      "Epoch: [5][0/391]\tLoss 1.5784 (1.5784)\n",
      "Epoch: [5][100/391]\tLoss 1.4001 (1.5894)\n",
      "Epoch: [5][200/391]\tLoss 1.5308 (1.5928)\n",
      "Epoch: [5][300/391]\tLoss 1.7274 (1.5839)\n",
      "train result: Loss: 1.5701118607330322, Acc: 42.256\n",
      "\n",
      "Test: [0/79]\tLoss 1.2775 (1.2775)\tPrec@1 50.000 (50.000)\n",
      "Validation result: Loss: 1.4366562898635864, Acc: 47.75\n",
      "\n",
      "Epoch: [6][0/391]\tLoss 1.5435 (1.5435)\n",
      "Epoch: [6][100/391]\tLoss 1.4509 (1.5311)\n",
      "Epoch: [6][200/391]\tLoss 1.5475 (1.5162)\n",
      "Epoch: [6][300/391]\tLoss 1.6076 (1.5030)\n",
      "train result: Loss: 1.4950392974853515, Acc: 45.136\n",
      "\n",
      "Test: [0/79]\tLoss 1.4130 (1.4130)\tPrec@1 49.219 (49.219)\n",
      "Validation result: Loss: 1.3554676331520081, Acc: 50.41\n",
      "\n",
      "Epoch: [7][0/391]\tLoss 1.6343 (1.6343)\n",
      "Epoch: [7][100/391]\tLoss 1.3321 (1.4398)\n",
      "Epoch: [7][200/391]\tLoss 1.5385 (1.4405)\n",
      "Epoch: [7][300/391]\tLoss 1.5225 (1.4352)\n",
      "train result: Loss: 1.431775082130432, Acc: 47.898\n",
      "\n",
      "Test: [0/79]\tLoss 1.4542 (1.4542)\tPrec@1 41.406 (41.406)\n",
      "Validation result: Loss: 1.3280805231094361, Acc: 50.73\n",
      "\n",
      "Epoch: [8][0/391]\tLoss 1.4381 (1.4381)\n",
      "Epoch: [8][100/391]\tLoss 1.5398 (1.3765)\n",
      "Epoch: [8][200/391]\tLoss 1.3476 (1.3740)\n",
      "Epoch: [8][300/391]\tLoss 1.3932 (1.3719)\n",
      "train result: Loss: 1.37096240814209, Acc: 50.024\n",
      "\n",
      "Test: [0/79]\tLoss 1.1676 (1.1676)\tPrec@1 52.344 (52.344)\n",
      "Validation result: Loss: 1.2479162855148316, Acc: 55.43\n",
      "\n",
      "Epoch: [9][0/391]\tLoss 1.2936 (1.2936)\n",
      "Epoch: [9][100/391]\tLoss 1.1935 (1.3269)\n",
      "Epoch: [9][200/391]\tLoss 1.2646 (1.3264)\n",
      "Epoch: [9][300/391]\tLoss 1.3686 (1.3207)\n",
      "train result: Loss: 1.3113737188339234, Acc: 52.768\n",
      "\n",
      "Test: [0/79]\tLoss 1.0239 (1.0239)\tPrec@1 62.500 (62.500)\n",
      "Validation result: Loss: 1.2006016128540038, Acc: 56.99\n",
      "\n",
      "Epoch: [10][0/391]\tLoss 1.1283 (1.1283)\n",
      "Epoch: [10][100/391]\tLoss 1.2456 (1.2565)\n",
      "Epoch: [10][200/391]\tLoss 1.2297 (1.2652)\n",
      "Epoch: [10][300/391]\tLoss 1.1257 (1.2615)\n",
      "train result: Loss: 1.2608992029190063, Acc: 54.718\n",
      "\n",
      "Test: [0/79]\tLoss 1.2980 (1.2980)\tPrec@1 53.906 (53.906)\n",
      "Validation result: Loss: 1.1554200770378114, Acc: 59.03\n",
      "\n",
      "Epoch: [11][0/391]\tLoss 1.2819 (1.2819)\n",
      "Epoch: [11][100/391]\tLoss 1.2974 (1.2389)\n",
      "Epoch: [11][200/391]\tLoss 1.3357 (1.2259)\n",
      "Epoch: [11][300/391]\tLoss 1.1218 (1.2200)\n",
      "train result: Loss: 1.2115821105575562, Acc: 56.662\n",
      "\n",
      "Test: [0/79]\tLoss 1.0694 (1.0694)\tPrec@1 59.375 (59.375)\n",
      "Validation result: Loss: 1.090119030380249, Acc: 60.91\n",
      "\n",
      "Epoch: [12][0/391]\tLoss 1.1633 (1.1633)\n",
      "Epoch: [12][100/391]\tLoss 1.0232 (1.1606)\n",
      "Epoch: [12][200/391]\tLoss 1.2530 (1.1808)\n",
      "Epoch: [12][300/391]\tLoss 1.0477 (1.1763)\n",
      "train result: Loss: 1.1663918943023681, Acc: 58.494\n",
      "\n",
      "Test: [0/79]\tLoss 0.6741 (0.6741)\tPrec@1 75.000 (75.000)\n",
      "Validation result: Loss: 1.1325154415130616, Acc: 60.35\n",
      "\n",
      "Epoch: [13][0/391]\tLoss 1.3351 (1.3351)\n",
      "Epoch: [13][100/391]\tLoss 1.1684 (1.1336)\n",
      "Epoch: [13][200/391]\tLoss 1.1346 (1.1266)\n",
      "Epoch: [13][300/391]\tLoss 1.1287 (1.1290)\n",
      "train result: Loss: 1.1296605703353881, Acc: 60.012\n",
      "\n",
      "Test: [0/79]\tLoss 0.8605 (0.8605)\tPrec@1 70.312 (70.312)\n",
      "Validation result: Loss: 1.0369434769630432, Acc: 64.02\n",
      "\n",
      "Epoch: [14][0/391]\tLoss 1.0762 (1.0762)\n",
      "Epoch: [14][100/391]\tLoss 1.1752 (1.0677)\n",
      "Epoch: [14][200/391]\tLoss 1.2129 (1.0732)\n",
      "Epoch: [14][300/391]\tLoss 1.1795 (1.0718)\n",
      "train result: Loss: 1.0688563654708862, Acc: 62.064\n",
      "\n",
      "Test: [0/79]\tLoss 0.9968 (0.9968)\tPrec@1 61.719 (61.719)\n",
      "Validation result: Loss: 0.9977743712425232, Acc: 64.47\n",
      "\n",
      "Epoch: [15][0/391]\tLoss 1.1086 (1.1086)\n",
      "Epoch: [15][100/391]\tLoss 0.9390 (1.0489)\n",
      "Epoch: [15][200/391]\tLoss 1.0619 (1.0343)\n",
      "Epoch: [15][300/391]\tLoss 1.1185 (1.0331)\n",
      "train result: Loss: 1.035024757041931, Acc: 63.58\n",
      "\n",
      "Test: [0/79]\tLoss 0.9724 (0.9724)\tPrec@1 65.625 (65.625)\n",
      "Validation result: Loss: 1.028519149017334, Acc: 64.43\n",
      "\n",
      "Epoch: [16][0/391]\tLoss 1.1543 (1.1543)\n",
      "Epoch: [16][100/391]\tLoss 0.9943 (1.0163)\n",
      "Epoch: [16][200/391]\tLoss 0.9541 (1.0036)\n",
      "Epoch: [16][300/391]\tLoss 1.1095 (1.0003)\n",
      "train result: Loss: 0.9948270415496826, Acc: 64.998\n",
      "\n",
      "Test: [0/79]\tLoss 1.0769 (1.0769)\tPrec@1 60.938 (60.938)\n",
      "Validation result: Loss: 0.904790142250061, Acc: 68.69\n",
      "\n",
      "Epoch: [17][0/391]\tLoss 0.8560 (0.8560)\n",
      "Epoch: [17][100/391]\tLoss 1.0117 (0.9855)\n",
      "Epoch: [17][200/391]\tLoss 0.9826 (0.9792)\n",
      "Epoch: [17][300/391]\tLoss 0.8900 (0.9767)\n",
      "train result: Loss: 0.9744504378890991, Acc: 65.462\n",
      "\n",
      "Test: [0/79]\tLoss 0.9205 (0.9205)\tPrec@1 64.844 (64.844)\n",
      "Validation result: Loss: 0.8729954731941223, Acc: 69.21\n",
      "\n",
      "Epoch: [18][0/391]\tLoss 0.8759 (0.8759)\n",
      "Epoch: [18][100/391]\tLoss 0.8998 (0.9339)\n",
      "Epoch: [18][200/391]\tLoss 0.9515 (0.9475)\n",
      "Epoch: [18][300/391]\tLoss 0.9065 (0.9397)\n",
      "train result: Loss: 0.9374024829483032, Acc: 67.182\n",
      "\n",
      "Test: [0/79]\tLoss 0.7721 (0.7721)\tPrec@1 76.562 (76.562)\n",
      "Validation result: Loss: 0.8776038292884827, Acc: 69.37\n",
      "\n",
      "Epoch: [19][0/391]\tLoss 0.8638 (0.8638)\n",
      "Epoch: [19][100/391]\tLoss 0.8740 (0.9152)\n",
      "Epoch: [19][200/391]\tLoss 0.9932 (0.9104)\n",
      "Epoch: [19][300/391]\tLoss 0.9070 (0.9070)\n",
      "train result: Loss: 0.9036549067878723, Acc: 68.27\n",
      "\n",
      "Test: [0/79]\tLoss 0.9726 (0.9726)\tPrec@1 64.844 (64.844)\n",
      "Validation result: Loss: 0.8294529933929443, Acc: 71.02\n",
      "\n",
      "Epoch: [20][0/391]\tLoss 0.7805 (0.7805)\n",
      "Epoch: [20][100/391]\tLoss 0.7963 (0.8801)\n",
      "Epoch: [20][200/391]\tLoss 0.7753 (0.8765)\n",
      "Epoch: [20][300/391]\tLoss 0.9699 (0.8765)\n",
      "train result: Loss: 0.8736152961540222, Acc: 69.466\n",
      "\n",
      "Test: [0/79]\tLoss 0.9521 (0.9521)\tPrec@1 65.625 (65.625)\n",
      "Validation result: Loss: 0.799825733089447, Acc: 71.74\n",
      "\n",
      "Epoch: [21][0/391]\tLoss 0.6590 (0.6590)\n",
      "Epoch: [21][100/391]\tLoss 0.8903 (0.8493)\n",
      "Epoch: [21][200/391]\tLoss 0.7890 (0.8462)\n",
      "Epoch: [21][300/391]\tLoss 0.7093 (0.8497)\n",
      "train result: Loss: 0.847132295513153, Acc: 70.178\n",
      "\n",
      "Test: [0/79]\tLoss 0.9954 (0.9954)\tPrec@1 63.281 (63.281)\n",
      "Validation result: Loss: 0.8157205785751342, Acc: 71.65\n",
      "\n",
      "Epoch: [22][0/391]\tLoss 0.7399 (0.7399)\n",
      "Epoch: [22][100/391]\tLoss 0.7835 (0.8270)\n",
      "Epoch: [22][200/391]\tLoss 0.6349 (0.8200)\n",
      "Epoch: [22][300/391]\tLoss 0.9018 (0.8237)\n",
      "train result: Loss: 0.8202283237838746, Acc: 71.184\n",
      "\n",
      "Test: [0/79]\tLoss 0.7434 (0.7434)\tPrec@1 77.344 (77.344)\n",
      "Validation result: Loss: 0.7698505219459534, Acc: 72.92\n",
      "\n",
      "Epoch: [23][0/391]\tLoss 0.8071 (0.8071)\n",
      "Epoch: [23][100/391]\tLoss 0.8689 (0.8070)\n",
      "Epoch: [23][200/391]\tLoss 0.8653 (0.7968)\n",
      "Epoch: [23][300/391]\tLoss 0.7476 (0.7999)\n",
      "train result: Loss: 0.7980092226791382, Acc: 72.032\n",
      "\n",
      "Test: [0/79]\tLoss 0.8983 (0.8983)\tPrec@1 67.969 (67.969)\n",
      "Validation result: Loss: 0.7481510401725769, Acc: 73.96\n",
      "\n",
      "Epoch: [24][0/391]\tLoss 0.8227 (0.8227)\n",
      "Epoch: [24][100/391]\tLoss 0.8377 (0.7712)\n",
      "Epoch: [24][200/391]\tLoss 0.9877 (0.7732)\n",
      "Epoch: [24][300/391]\tLoss 0.7293 (0.7781)\n",
      "train result: Loss: 0.7753393557357788, Acc: 72.726\n",
      "\n",
      "Test: [0/79]\tLoss 0.4838 (0.4838)\tPrec@1 79.688 (79.688)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation result: Loss: 0.7685459285736084, Acc: 72.79\n",
      "\n",
      "Epoch: [25][0/391]\tLoss 0.8290 (0.8290)\n",
      "Epoch: [25][100/391]\tLoss 0.6420 (0.7573)\n",
      "Epoch: [25][200/391]\tLoss 0.8139 (0.7477)\n",
      "Epoch: [25][300/391]\tLoss 0.8026 (0.7507)\n",
      "train result: Loss: 0.7527597517395019, Acc: 73.674\n",
      "\n",
      "Test: [0/79]\tLoss 1.0285 (1.0285)\tPrec@1 66.406 (66.406)\n",
      "Validation result: Loss: 0.7554134937286376, Acc: 73.7\n",
      "\n",
      "Epoch: [26][0/391]\tLoss 0.6555 (0.6555)\n",
      "Epoch: [26][100/391]\tLoss 0.6983 (0.7217)\n",
      "Epoch: [26][200/391]\tLoss 0.6257 (0.7295)\n",
      "Epoch: [26][300/391]\tLoss 0.5971 (0.7233)\n",
      "train result: Loss: 0.725332568283081, Acc: 74.568\n",
      "\n",
      "Test: [0/79]\tLoss 0.9188 (0.9188)\tPrec@1 67.969 (67.969)\n",
      "Validation result: Loss: 0.6838546440124512, Acc: 76.12\n",
      "\n",
      "Epoch: [27][0/391]\tLoss 0.6317 (0.6317)\n",
      "Epoch: [27][100/391]\tLoss 0.6755 (0.7081)\n",
      "Epoch: [27][200/391]\tLoss 0.8115 (0.7143)\n",
      "Epoch: [27][300/391]\tLoss 0.8650 (0.7130)\n",
      "train result: Loss: 0.7123863801956177, Acc: 75.128\n",
      "\n",
      "Test: [0/79]\tLoss 0.6616 (0.6616)\tPrec@1 73.438 (73.438)\n",
      "Validation result: Loss: 0.6908797510147094, Acc: 75.8\n",
      "\n",
      "Epoch: [28][0/391]\tLoss 0.7439 (0.7439)\n",
      "Epoch: [28][100/391]\tLoss 0.7033 (0.6942)\n",
      "Epoch: [28][200/391]\tLoss 0.7621 (0.6935)\n",
      "Epoch: [28][300/391]\tLoss 0.5959 (0.6890)\n",
      "train result: Loss: 0.6916591863822937, Acc: 75.846\n",
      "\n",
      "Test: [0/79]\tLoss 0.6102 (0.6102)\tPrec@1 78.906 (78.906)\n",
      "Validation result: Loss: 0.6572188374996185, Acc: 77.07\n",
      "\n",
      "Epoch: [29][0/391]\tLoss 0.6914 (0.6914)\n",
      "Epoch: [29][100/391]\tLoss 0.6508 (0.6687)\n",
      "Epoch: [29][200/391]\tLoss 0.4930 (0.6658)\n",
      "Epoch: [29][300/391]\tLoss 0.7235 (0.6698)\n",
      "train result: Loss: 0.6737195469093322, Acc: 76.282\n",
      "\n",
      "Test: [0/79]\tLoss 0.7804 (0.7804)\tPrec@1 74.219 (74.219)\n",
      "Validation result: Loss: 0.6603089056968688, Acc: 76.99\n",
      "\n",
      "Epoch: [30][0/391]\tLoss 0.7604 (0.7604)\n",
      "Epoch: [30][100/391]\tLoss 0.5047 (0.6570)\n",
      "Epoch: [30][200/391]\tLoss 0.7696 (0.6561)\n",
      "Epoch: [30][300/391]\tLoss 0.5618 (0.6542)\n",
      "train result: Loss: 0.6531074866104126, Acc: 77.084\n",
      "\n",
      "Test: [0/79]\tLoss 0.6415 (0.6415)\tPrec@1 80.469 (80.469)\n",
      "Validation result: Loss: 0.6350073834419251, Acc: 78.07\n",
      "\n",
      "Epoch: [31][0/391]\tLoss 0.6532 (0.6532)\n",
      "Epoch: [31][100/391]\tLoss 0.6363 (0.6258)\n",
      "Epoch: [31][200/391]\tLoss 0.7003 (0.6357)\n",
      "Epoch: [31][300/391]\tLoss 0.6125 (0.6382)\n",
      "train result: Loss: 0.6379736965179443, Acc: 77.72\n",
      "\n",
      "Test: [0/79]\tLoss 0.6689 (0.6689)\tPrec@1 78.906 (78.906)\n",
      "Validation result: Loss: 0.6192078143119812, Acc: 78.67\n",
      "\n",
      "Epoch: [32][0/391]\tLoss 0.5829 (0.5829)\n",
      "Epoch: [32][100/391]\tLoss 0.5036 (0.6208)\n",
      "Epoch: [32][200/391]\tLoss 0.6314 (0.6226)\n",
      "Epoch: [32][300/391]\tLoss 0.6977 (0.6289)\n",
      "train result: Loss: 0.6254845943260193, Acc: 78.174\n",
      "\n",
      "Test: [0/79]\tLoss 0.6784 (0.6784)\tPrec@1 77.344 (77.344)\n",
      "Validation result: Loss: 0.6001579714298249, Acc: 79.25\n",
      "\n",
      "Epoch: [33][0/391]\tLoss 0.6029 (0.6029)\n",
      "Epoch: [33][100/391]\tLoss 0.6742 (0.6261)\n",
      "Epoch: [33][200/391]\tLoss 0.6361 (0.6209)\n",
      "Epoch: [33][300/391]\tLoss 0.5652 (0.6203)\n",
      "train result: Loss: 0.6196914316177368, Acc: 78.358\n",
      "\n",
      "Test: [0/79]\tLoss 0.7104 (0.7104)\tPrec@1 75.000 (75.000)\n",
      "Validation result: Loss: 0.5944290717124939, Acc: 79.17\n",
      "\n",
      "Epoch: [34][0/391]\tLoss 0.6261 (0.6261)\n",
      "Epoch: [34][100/391]\tLoss 0.6217 (0.6021)\n",
      "Epoch: [34][200/391]\tLoss 0.5793 (0.5999)\n",
      "Epoch: [34][300/391]\tLoss 0.5239 (0.6053)\n",
      "train result: Loss: 0.6052989170074463, Acc: 78.878\n",
      "\n",
      "Test: [0/79]\tLoss 0.9545 (0.9545)\tPrec@1 66.406 (66.406)\n",
      "Validation result: Loss: 0.6093446617126465, Acc: 78.96\n",
      "\n",
      "Epoch: [35][0/391]\tLoss 0.8286 (0.8286)\n",
      "Epoch: [35][100/391]\tLoss 0.5585 (0.5725)\n",
      "Epoch: [35][200/391]\tLoss 0.5712 (0.5813)\n",
      "Epoch: [35][300/391]\tLoss 0.5104 (0.5878)\n",
      "train result: Loss: 0.5885073795700073, Acc: 79.352\n",
      "\n",
      "Test: [0/79]\tLoss 0.6819 (0.6819)\tPrec@1 78.906 (78.906)\n",
      "Validation result: Loss: 0.58371749958992, Acc: 79.61\n",
      "\n",
      "Epoch: [36][0/391]\tLoss 0.5351 (0.5351)\n",
      "Epoch: [36][100/391]\tLoss 0.6199 (0.5767)\n",
      "Epoch: [36][200/391]\tLoss 0.5392 (0.5731)\n",
      "Epoch: [36][300/391]\tLoss 0.6443 (0.5778)\n",
      "train result: Loss: 0.5803354231071473, Acc: 79.782\n",
      "\n",
      "Test: [0/79]\tLoss 0.7817 (0.7817)\tPrec@1 75.781 (75.781)\n",
      "Validation result: Loss: 0.5800018691062927, Acc: 79.99\n",
      "\n",
      "Epoch: [37][0/391]\tLoss 0.5937 (0.5937)\n",
      "Epoch: [37][100/391]\tLoss 0.4575 (0.5541)\n",
      "Epoch: [37][200/391]\tLoss 0.4432 (0.5653)\n",
      "Epoch: [37][300/391]\tLoss 0.6243 (0.5676)\n",
      "train result: Loss: 0.5691984065055847, Acc: 80.154\n",
      "\n",
      "Test: [0/79]\tLoss 0.6174 (0.6174)\tPrec@1 79.688 (79.688)\n",
      "Validation result: Loss: 0.5623118915081025, Acc: 80.35\n",
      "\n",
      "Epoch: [38][0/391]\tLoss 0.5603 (0.5603)\n",
      "Epoch: [38][100/391]\tLoss 0.5886 (0.5522)\n",
      "Epoch: [38][200/391]\tLoss 0.5379 (0.5542)\n",
      "Epoch: [38][300/391]\tLoss 0.6986 (0.5546)\n",
      "train result: Loss: 0.5521869271087646, Acc: 80.678\n",
      "\n",
      "Test: [0/79]\tLoss 0.6033 (0.6033)\tPrec@1 78.906 (78.906)\n",
      "Validation result: Loss: 0.5732948932647705, Acc: 79.91\n",
      "\n",
      "Epoch: [39][0/391]\tLoss 0.5655 (0.5655)\n",
      "Epoch: [39][100/391]\tLoss 0.5911 (0.5375)\n",
      "Epoch: [39][200/391]\tLoss 0.5423 (0.5311)\n",
      "Epoch: [39][300/391]\tLoss 0.4119 (0.5380)\n",
      "train result: Loss: 0.5387004400444031, Acc: 81.074\n",
      "\n",
      "Test: [0/79]\tLoss 0.5857 (0.5857)\tPrec@1 81.250 (81.250)\n",
      "Validation result: Loss: 0.5863348945140838, Acc: 79.68\n",
      "\n",
      "Epoch: [40][0/391]\tLoss 0.3821 (0.3821)\n",
      "Epoch: [40][100/391]\tLoss 0.4960 (0.5313)\n",
      "Epoch: [40][200/391]\tLoss 0.5448 (0.5340)\n",
      "Epoch: [40][300/391]\tLoss 0.6431 (0.5349)\n",
      "train result: Loss: 0.5344296131896973, Acc: 81.502\n",
      "\n",
      "Test: [0/79]\tLoss 0.7421 (0.7421)\tPrec@1 78.125 (78.125)\n",
      "Validation result: Loss: 0.550616615819931, Acc: 80.85\n",
      "\n",
      "Epoch: [41][0/391]\tLoss 0.4200 (0.4200)\n",
      "Epoch: [41][100/391]\tLoss 0.5225 (0.5195)\n",
      "Epoch: [41][200/391]\tLoss 0.3774 (0.5132)\n",
      "Epoch: [41][300/391]\tLoss 0.6974 (0.5180)\n",
      "train result: Loss: 0.5234967562866211, Acc: 81.752\n",
      "\n",
      "Test: [0/79]\tLoss 0.6892 (0.6892)\tPrec@1 77.344 (77.344)\n",
      "Validation result: Loss: 0.5570093809127807, Acc: 80.53\n",
      "\n",
      "Epoch: [42][0/391]\tLoss 0.6002 (0.6002)\n",
      "Epoch: [42][100/391]\tLoss 0.4932 (0.5132)\n",
      "Epoch: [42][200/391]\tLoss 0.7086 (0.5206)\n",
      "Epoch: [42][300/391]\tLoss 0.5545 (0.5194)\n",
      "train result: Loss: 0.5184308581352234, Acc: 81.928\n",
      "\n",
      "Test: [0/79]\tLoss 0.4725 (0.4725)\tPrec@1 80.469 (80.469)\n",
      "Validation result: Loss: 0.5462249814748764, Acc: 81.09\n",
      "\n",
      "Epoch: [43][0/391]\tLoss 0.4585 (0.4585)\n",
      "Epoch: [43][100/391]\tLoss 0.4588 (0.5140)\n",
      "Epoch: [43][200/391]\tLoss 0.4670 (0.5081)\n",
      "Epoch: [43][300/391]\tLoss 0.5510 (0.5105)\n",
      "train result: Loss: 0.5112200935173035, Acc: 82.174\n",
      "\n",
      "Test: [0/79]\tLoss 0.4550 (0.4550)\tPrec@1 84.375 (84.375)\n",
      "Validation result: Loss: 0.5880831851005555, Acc: 79.61\n",
      "\n",
      "Epoch: [44][0/391]\tLoss 0.5149 (0.5149)\n",
      "Epoch: [44][100/391]\tLoss 0.4567 (0.4909)\n",
      "Epoch: [44][200/391]\tLoss 0.3395 (0.4897)\n",
      "Epoch: [44][300/391]\tLoss 0.4865 (0.4934)\n",
      "train result: Loss: 0.49770374267578127, Acc: 82.668\n",
      "\n",
      "Test: [0/79]\tLoss 0.7048 (0.7048)\tPrec@1 75.000 (75.000)\n",
      "Validation result: Loss: 0.532601344537735, Acc: 81.65\n",
      "\n",
      "Epoch: [45][0/391]\tLoss 0.3585 (0.3585)\n",
      "Epoch: [45][100/391]\tLoss 0.5523 (0.4862)\n",
      "Epoch: [45][200/391]\tLoss 0.4827 (0.4891)\n",
      "Epoch: [45][300/391]\tLoss 0.3705 (0.4974)\n",
      "train result: Loss: 0.4988020970058441, Acc: 82.702\n",
      "\n",
      "Test: [0/79]\tLoss 0.6554 (0.6554)\tPrec@1 72.656 (72.656)\n",
      "Validation result: Loss: 0.5529019620418548, Acc: 80.91\n",
      "\n",
      "Epoch: [46][0/391]\tLoss 0.3545 (0.3545)\n",
      "Epoch: [46][100/391]\tLoss 0.5199 (0.4750)\n",
      "Epoch: [46][200/391]\tLoss 0.4361 (0.4740)\n",
      "Epoch: [46][300/391]\tLoss 0.4564 (0.4793)\n",
      "train result: Loss: 0.4813941881752014, Acc: 83.19\n",
      "\n",
      "Test: [0/79]\tLoss 0.6302 (0.6302)\tPrec@1 78.906 (78.906)\n",
      "Validation result: Loss: 0.5262363924980163, Acc: 81.93\n",
      "\n",
      "Epoch: [47][0/391]\tLoss 0.5012 (0.5012)\n",
      "Epoch: [47][100/391]\tLoss 0.4007 (0.4689)\n",
      "Epoch: [47][200/391]\tLoss 0.6148 (0.4742)\n",
      "Epoch: [47][300/391]\tLoss 0.5476 (0.4772)\n",
      "train result: Loss: 0.47577183345794677, Acc: 83.482\n",
      "\n",
      "Test: [0/79]\tLoss 0.5934 (0.5934)\tPrec@1 78.906 (78.906)\n",
      "Validation result: Loss: 0.5285727572441101, Acc: 81.62\n",
      "\n",
      "Epoch: [48][0/391]\tLoss 0.3037 (0.3037)\n",
      "Epoch: [48][100/391]\tLoss 0.5306 (0.4713)\n",
      "Epoch: [48][200/391]\tLoss 0.4691 (0.4761)\n",
      "Epoch: [48][300/391]\tLoss 0.3694 (0.4764)\n",
      "train result: Loss: 0.4779528943729401, Acc: 83.292\n",
      "\n",
      "Test: [0/79]\tLoss 0.5897 (0.5897)\tPrec@1 82.031 (82.031)\n",
      "Validation result: Loss: 0.523823241853714, Acc: 81.76\n",
      "\n",
      "Epoch: [49][0/391]\tLoss 0.4140 (0.4140)\n",
      "Epoch: [49][100/391]\tLoss 0.4989 (0.4690)\n",
      "Epoch: [49][200/391]\tLoss 0.3352 (0.4642)\n",
      "Epoch: [49][300/391]\tLoss 0.5656 (0.4652)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train result: Loss: 0.46442218435287475, Acc: 83.794\n",
      "\n",
      "Test: [0/79]\tLoss 0.6411 (0.6411)\tPrec@1 79.688 (79.688)\n",
      "Validation result: Loss: 0.5228497806549073, Acc: 82.22\n",
      "\n",
      "Epoch: [50][0/391]\tLoss 0.4411 (0.4411)\n",
      "Epoch: [50][100/391]\tLoss 0.4892 (0.4438)\n",
      "Epoch: [50][200/391]\tLoss 0.3953 (0.4505)\n",
      "Epoch: [50][300/391]\tLoss 0.5606 (0.4547)\n",
      "train result: Loss: 0.45459284158706664, Acc: 84.16\n",
      "\n",
      "Test: [0/79]\tLoss 0.6808 (0.6808)\tPrec@1 77.344 (77.344)\n",
      "Validation result: Loss: 0.5045527636051178, Acc: 82.37\n",
      "\n",
      "Epoch: [51][0/391]\tLoss 0.2996 (0.2996)\n",
      "Epoch: [51][100/391]\tLoss 0.5067 (0.4224)\n",
      "Epoch: [51][200/391]\tLoss 0.4415 (0.4323)\n",
      "Epoch: [51][300/391]\tLoss 0.4107 (0.4427)\n",
      "train result: Loss: 0.44816823532104494, Acc: 84.27\n",
      "\n",
      "Test: [0/79]\tLoss 0.6669 (0.6669)\tPrec@1 78.125 (78.125)\n",
      "Validation result: Loss: 0.5211770203590393, Acc: 82.11\n",
      "\n",
      "Epoch: [52][0/391]\tLoss 0.5569 (0.5569)\n",
      "Epoch: [52][100/391]\tLoss 0.3899 (0.4492)\n",
      "Epoch: [52][200/391]\tLoss 0.4776 (0.4434)\n",
      "Epoch: [52][300/391]\tLoss 0.4441 (0.4455)\n",
      "train result: Loss: 0.4431091530418396, Acc: 84.616\n",
      "\n",
      "Test: [0/79]\tLoss 0.4675 (0.4675)\tPrec@1 82.031 (82.031)\n",
      "Validation result: Loss: 0.5066778219223023, Acc: 82.33\n",
      "\n",
      "Epoch: [53][0/391]\tLoss 0.5998 (0.5998)\n",
      "Epoch: [53][100/391]\tLoss 0.5078 (0.4324)\n",
      "Epoch: [53][200/391]\tLoss 0.4874 (0.4393)\n",
      "Epoch: [53][300/391]\tLoss 0.3167 (0.4393)\n",
      "train result: Loss: 0.4400547706604004, Acc: 84.45\n",
      "\n",
      "Test: [0/79]\tLoss 0.8281 (0.8281)\tPrec@1 74.219 (74.219)\n",
      "Validation result: Loss: 0.5100108244419098, Acc: 82.48\n",
      "\n",
      "Epoch: [54][0/391]\tLoss 0.3276 (0.3276)\n",
      "Epoch: [54][100/391]\tLoss 0.3524 (0.4280)\n",
      "Epoch: [54][200/391]\tLoss 0.5056 (0.4300)\n",
      "Epoch: [54][300/391]\tLoss 0.4167 (0.4347)\n",
      "train result: Loss: 0.4360452958869934, Acc: 84.912\n",
      "\n",
      "Test: [0/79]\tLoss 0.6751 (0.6751)\tPrec@1 75.781 (75.781)\n",
      "Validation result: Loss: 0.5170104852676392, Acc: 82.18\n",
      "\n",
      "Epoch: [55][0/391]\tLoss 0.4805 (0.4805)\n",
      "Epoch: [55][100/391]\tLoss 0.3700 (0.4261)\n",
      "Epoch: [55][200/391]\tLoss 0.3598 (0.4268)\n",
      "Epoch: [55][300/391]\tLoss 0.4845 (0.4310)\n",
      "train result: Loss: 0.43164307797431944, Acc: 84.92\n",
      "\n",
      "Test: [0/79]\tLoss 0.4973 (0.4973)\tPrec@1 81.250 (81.250)\n",
      "Validation result: Loss: 0.540239206647873, Acc: 81.4\n",
      "\n",
      "Epoch: [56][0/391]\tLoss 0.4020 (0.4020)\n",
      "Epoch: [56][100/391]\tLoss 0.2061 (0.4058)\n",
      "Epoch: [56][200/391]\tLoss 0.3516 (0.4135)\n",
      "Epoch: [56][300/391]\tLoss 0.4877 (0.4196)\n",
      "train result: Loss: 0.4237313326835632, Acc: 85.136\n",
      "\n",
      "Test: [0/79]\tLoss 0.8239 (0.8239)\tPrec@1 75.000 (75.000)\n",
      "Validation result: Loss: 0.529436462688446, Acc: 81.78\n",
      "\n",
      "Epoch: [57][0/391]\tLoss 0.3332 (0.3332)\n",
      "Epoch: [57][100/391]\tLoss 0.3941 (0.4116)\n",
      "Epoch: [57][200/391]\tLoss 0.3948 (0.4139)\n",
      "Epoch: [57][300/391]\tLoss 0.3970 (0.4154)\n",
      "train result: Loss: 0.41909722551345824, Acc: 85.388\n",
      "\n",
      "Test: [0/79]\tLoss 0.9178 (0.9178)\tPrec@1 72.656 (72.656)\n",
      "Validation result: Loss: 0.5178134618520737, Acc: 82.61\n",
      "\n",
      "Epoch: [58][0/391]\tLoss 0.5419 (0.5419)\n",
      "Epoch: [58][100/391]\tLoss 0.2193 (0.4074)\n",
      "Epoch: [58][200/391]\tLoss 0.4179 (0.4100)\n",
      "Epoch: [58][300/391]\tLoss 0.3105 (0.4109)\n",
      "train result: Loss: 0.41119212566375735, Acc: 85.796\n",
      "\n",
      "Test: [0/79]\tLoss 0.5352 (0.5352)\tPrec@1 82.031 (82.031)\n",
      "Validation result: Loss: 0.5325822285652161, Acc: 81.83\n",
      "\n",
      "Epoch: [59][0/391]\tLoss 0.3744 (0.3744)\n",
      "Epoch: [59][100/391]\tLoss 0.2899 (0.4077)\n",
      "Epoch: [59][200/391]\tLoss 0.4240 (0.4082)\n",
      "Epoch: [59][300/391]\tLoss 0.4824 (0.4112)\n",
      "train result: Loss: 0.41140278831481936, Acc: 85.81\n",
      "\n",
      "Test: [0/79]\tLoss 0.8739 (0.8739)\tPrec@1 74.219 (74.219)\n",
      "Validation result: Loss: 0.546623904466629, Acc: 81.95\n",
      "\n",
      "Epoch: [60][0/391]\tLoss 0.3541 (0.3541)\n",
      "Epoch: [60][100/391]\tLoss 0.4294 (0.3925)\n",
      "Epoch: [60][200/391]\tLoss 0.2985 (0.3984)\n",
      "Epoch: [60][300/391]\tLoss 0.3740 (0.4066)\n",
      "train result: Loss: 0.4093556350326538, Acc: 85.728\n",
      "\n",
      "Test: [0/79]\tLoss 0.5932 (0.5932)\tPrec@1 82.812 (82.812)\n",
      "Validation result: Loss: 0.5020375334262848, Acc: 82.33\n",
      "\n",
      "Epoch: [61][0/391]\tLoss 0.5305 (0.5305)\n",
      "Epoch: [61][100/391]\tLoss 0.2659 (0.3800)\n",
      "Epoch: [61][200/391]\tLoss 0.3181 (0.3936)\n",
      "Epoch: [61][300/391]\tLoss 0.3125 (0.3995)\n",
      "train result: Loss: 0.3961794689464569, Acc: 86.274\n",
      "\n",
      "Test: [0/79]\tLoss 0.9056 (0.9056)\tPrec@1 69.531 (69.531)\n",
      "Validation result: Loss: 0.525022940826416, Acc: 82.44\n",
      "\n",
      "Epoch: [62][0/391]\tLoss 0.3541 (0.3541)\n",
      "Epoch: [62][100/391]\tLoss 0.4446 (0.3991)\n",
      "Epoch: [62][200/391]\tLoss 0.5290 (0.3978)\n",
      "Epoch: [62][300/391]\tLoss 0.3911 (0.4003)\n",
      "train result: Loss: 0.4024280902004242, Acc: 85.938\n",
      "\n",
      "Test: [0/79]\tLoss 0.6561 (0.6561)\tPrec@1 79.688 (79.688)\n",
      "Validation result: Loss: 0.4905245849609375, Acc: 83.59\n",
      "\n",
      "Epoch: [63][0/391]\tLoss 0.3460 (0.3460)\n",
      "Epoch: [63][100/391]\tLoss 0.3665 (0.3787)\n",
      "Epoch: [63][200/391]\tLoss 0.3305 (0.3801)\n",
      "Epoch: [63][300/391]\tLoss 0.4638 (0.3824)\n",
      "train result: Loss: 0.3886136345291138, Acc: 86.582\n",
      "\n",
      "Test: [0/79]\tLoss 0.8265 (0.8265)\tPrec@1 73.438 (73.438)\n",
      "Validation result: Loss: 0.4993017021179199, Acc: 83.55\n",
      "\n",
      "Epoch: [64][0/391]\tLoss 0.3730 (0.3730)\n",
      "Epoch: [64][100/391]\tLoss 0.4501 (0.3659)\n",
      "Epoch: [64][200/391]\tLoss 0.4821 (0.3830)\n",
      "Epoch: [64][300/391]\tLoss 0.3651 (0.3909)\n",
      "train result: Loss: 0.39260070516586304, Acc: 86.088\n",
      "\n",
      "Test: [0/79]\tLoss 0.4738 (0.4738)\tPrec@1 85.938 (85.938)\n",
      "Validation result: Loss: 0.5100482927799225, Acc: 82.52\n",
      "\n",
      "Epoch: [65][0/391]\tLoss 0.3051 (0.3051)\n",
      "Epoch: [65][100/391]\tLoss 0.5147 (0.3663)\n",
      "Epoch: [65][200/391]\tLoss 0.2667 (0.3708)\n",
      "Epoch: [65][300/391]\tLoss 0.4129 (0.3725)\n",
      "train result: Loss: 0.37796258957862855, Acc: 86.786\n",
      "\n",
      "Test: [0/79]\tLoss 0.5672 (0.5672)\tPrec@1 82.031 (82.031)\n",
      "Validation result: Loss: 0.5027381880283356, Acc: 82.64\n",
      "\n",
      "Epoch: [66][0/391]\tLoss 0.2757 (0.2757)\n",
      "Epoch: [66][100/391]\tLoss 0.4607 (0.3616)\n",
      "Epoch: [66][200/391]\tLoss 0.2466 (0.3715)\n",
      "Epoch: [66][300/391]\tLoss 0.4639 (0.3758)\n",
      "train result: Loss: 0.37956707306861875, Acc: 86.576\n",
      "\n",
      "Test: [0/79]\tLoss 0.7558 (0.7558)\tPrec@1 78.906 (78.906)\n",
      "Validation result: Loss: 0.5061736827850342, Acc: 82.88\n",
      "\n",
      "Epoch: [67][0/391]\tLoss 0.4910 (0.4910)\n",
      "Epoch: [67][100/391]\tLoss 0.3779 (0.3667)\n",
      "Epoch: [67][200/391]\tLoss 0.3597 (0.3822)\n",
      "Epoch: [67][300/391]\tLoss 0.3845 (0.3810)\n",
      "train result: Loss: 0.38250999468803404, Acc: 86.686\n",
      "\n",
      "Test: [0/79]\tLoss 0.5545 (0.5545)\tPrec@1 80.469 (80.469)\n",
      "Validation result: Loss: 0.5004008821010589, Acc: 82.9\n",
      "\n",
      "Epoch: [68][0/391]\tLoss 0.3341 (0.3341)\n",
      "Epoch: [68][100/391]\tLoss 0.5015 (0.3720)\n",
      "Epoch: [68][200/391]\tLoss 0.4477 (0.3732)\n",
      "Epoch: [68][300/391]\tLoss 0.5121 (0.3715)\n",
      "train result: Loss: 0.37387047171592713, Acc: 86.884\n",
      "\n",
      "Test: [0/79]\tLoss 0.4177 (0.4177)\tPrec@1 88.281 (88.281)\n",
      "Validation result: Loss: 0.49103598308563234, Acc: 83.46\n",
      "\n",
      "Epoch: [69][0/391]\tLoss 0.3010 (0.3010)\n",
      "Epoch: [69][100/391]\tLoss 0.4728 (0.3535)\n",
      "Epoch: [69][200/391]\tLoss 0.3131 (0.3680)\n",
      "Epoch: [69][300/391]\tLoss 0.4515 (0.3696)\n",
      "train result: Loss: 0.37418161036491393, Acc: 86.982\n",
      "\n",
      "Test: [0/79]\tLoss 0.5643 (0.5643)\tPrec@1 81.250 (81.250)\n",
      "Validation result: Loss: 0.4996787549972534, Acc: 83.05\n",
      "\n",
      "Epoch: [70][0/391]\tLoss 0.3000 (0.3000)\n",
      "Epoch: [70][100/391]\tLoss 0.4274 (0.3583)\n",
      "Epoch: [70][200/391]\tLoss 0.3875 (0.3605)\n",
      "Epoch: [70][300/391]\tLoss 0.3974 (0.3630)\n",
      "train result: Loss: 0.3649235995864868, Acc: 87.312\n",
      "\n",
      "Test: [0/79]\tLoss 0.5790 (0.5790)\tPrec@1 80.469 (80.469)\n",
      "Validation result: Loss: 0.48181632866859436, Acc: 84.14\n",
      "\n",
      "Epoch: [71][0/391]\tLoss 0.2106 (0.2106)\n",
      "Epoch: [71][100/391]\tLoss 0.3188 (0.3574)\n",
      "Epoch: [71][200/391]\tLoss 0.5632 (0.3584)\n",
      "Epoch: [71][300/391]\tLoss 0.4766 (0.3626)\n",
      "train result: Loss: 0.36660854479789734, Acc: 87.214\n",
      "\n",
      "Test: [0/79]\tLoss 0.7523 (0.7523)\tPrec@1 77.344 (77.344)\n",
      "Validation result: Loss: 0.4762735943317413, Acc: 83.72\n",
      "\n",
      "Epoch: [72][0/391]\tLoss 0.3839 (0.3839)\n",
      "Epoch: [72][100/391]\tLoss 0.3336 (0.3469)\n",
      "Epoch: [72][200/391]\tLoss 0.3463 (0.3548)\n",
      "Epoch: [72][300/391]\tLoss 0.3228 (0.3575)\n",
      "train result: Loss: 0.361050205821991, Acc: 87.468\n",
      "\n",
      "Test: [0/79]\tLoss 0.6560 (0.6560)\tPrec@1 83.594 (83.594)\n",
      "Validation result: Loss: 0.5081519749641419, Acc: 82.5\n",
      "\n",
      "Epoch: [73][0/391]\tLoss 0.2856 (0.2856)\n",
      "Epoch: [73][100/391]\tLoss 0.3180 (0.3580)\n",
      "Epoch: [73][200/391]\tLoss 0.3320 (0.3588)\n",
      "Epoch: [73][300/391]\tLoss 0.3146 (0.3612)\n",
      "train result: Loss: 0.3627589374065399, Acc: 87.394\n",
      "\n",
      "Test: [0/79]\tLoss 0.6529 (0.6529)\tPrec@1 75.000 (75.000)\n",
      "Validation result: Loss: 0.544510986328125, Acc: 82.48\n",
      "\n",
      "Epoch: [74][0/391]\tLoss 0.4283 (0.4283)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [74][100/391]\tLoss 0.4509 (0.3383)\n",
      "Epoch: [74][200/391]\tLoss 0.2806 (0.3440)\n",
      "Epoch: [74][300/391]\tLoss 0.3499 (0.3467)\n",
      "train result: Loss: 0.35137681741714477, Acc: 87.822\n",
      "\n",
      "Test: [0/79]\tLoss 0.6850 (0.6850)\tPrec@1 76.562 (76.562)\n",
      "Validation result: Loss: 0.4883403896808624, Acc: 83.7\n",
      "\n",
      "Epoch: [75][0/391]\tLoss 0.3745 (0.3745)\n",
      "Epoch: [75][100/391]\tLoss 0.2753 (0.3283)\n",
      "Epoch: [75][200/391]\tLoss 0.4370 (0.3422)\n",
      "Epoch: [75][300/391]\tLoss 0.3740 (0.3458)\n",
      "train result: Loss: 0.34742406455039976, Acc: 87.858\n",
      "\n",
      "Test: [0/79]\tLoss 0.5470 (0.5470)\tPrec@1 83.594 (83.594)\n",
      "Validation result: Loss: 0.49209948830604555, Acc: 83.39\n",
      "\n",
      "Epoch: [76][0/391]\tLoss 0.3582 (0.3582)\n",
      "Epoch: [76][100/391]\tLoss 0.4418 (0.3417)\n",
      "Epoch: [76][200/391]\tLoss 0.4631 (0.3586)\n",
      "Epoch: [76][300/391]\tLoss 0.2996 (0.3553)\n",
      "train result: Loss: 0.3535060407352448, Acc: 87.904\n",
      "\n",
      "Test: [0/79]\tLoss 0.7123 (0.7123)\tPrec@1 76.562 (76.562)\n",
      "Validation result: Loss: 0.5182803246498108, Acc: 82.98\n",
      "\n",
      "Epoch: [77][0/391]\tLoss 0.4068 (0.4068)\n",
      "Epoch: [77][100/391]\tLoss 0.2969 (0.3374)\n",
      "Epoch: [77][200/391]\tLoss 0.4452 (0.3424)\n",
      "Epoch: [77][300/391]\tLoss 0.2516 (0.3392)\n",
      "train result: Loss: 0.3417788305234909, Acc: 88.074\n",
      "\n",
      "Test: [0/79]\tLoss 0.5125 (0.5125)\tPrec@1 85.156 (85.156)\n",
      "Validation result: Loss: 0.4949592625141144, Acc: 83.7\n",
      "\n",
      "Epoch: [78][0/391]\tLoss 0.2989 (0.2989)\n",
      "Epoch: [78][100/391]\tLoss 0.2826 (0.3362)\n",
      "Epoch: [78][200/391]\tLoss 0.2898 (0.3392)\n",
      "Epoch: [78][300/391]\tLoss 0.3966 (0.3386)\n",
      "train result: Loss: 0.3413978954124451, Acc: 88.164\n",
      "\n",
      "Test: [0/79]\tLoss 0.7588 (0.7588)\tPrec@1 76.562 (76.562)\n",
      "Validation result: Loss: 0.5109515288352966, Acc: 83.07\n",
      "\n",
      "Epoch: [79][0/391]\tLoss 0.1769 (0.1769)\n",
      "Epoch: [79][100/391]\tLoss 0.2711 (0.3297)\n",
      "Epoch: [79][200/391]\tLoss 0.4231 (0.3338)\n",
      "Epoch: [79][300/391]\tLoss 0.2918 (0.3375)\n",
      "train result: Loss: 0.34209459913253787, Acc: 88.112\n",
      "\n",
      "Test: [0/79]\tLoss 0.7124 (0.7124)\tPrec@1 76.562 (76.562)\n",
      "Validation result: Loss: 0.4985693735599518, Acc: 83.53\n",
      "\n",
      "Epoch: [80][0/391]\tLoss 0.2930 (0.2930)\n",
      "Epoch: [80][100/391]\tLoss 0.4409 (0.3330)\n",
      "Epoch: [80][200/391]\tLoss 0.3240 (0.3397)\n",
      "Epoch: [80][300/391]\tLoss 0.3078 (0.3431)\n",
      "train result: Loss: 0.3437065986633301, Acc: 87.992\n",
      "\n",
      "Test: [0/79]\tLoss 0.6078 (0.6078)\tPrec@1 82.031 (82.031)\n",
      "Validation result: Loss: 0.5028751318931579, Acc: 84.11\n",
      "\n",
      "Epoch: [81][0/391]\tLoss 0.3123 (0.3123)\n",
      "Epoch: [81][100/391]\tLoss 0.3175 (0.3272)\n",
      "Epoch: [81][200/391]\tLoss 0.3499 (0.3359)\n",
      "Epoch: [81][300/391]\tLoss 0.3876 (0.3377)\n",
      "train result: Loss: 0.3389972465515137, Acc: 88.12\n",
      "\n",
      "Test: [0/79]\tLoss 0.5351 (0.5351)\tPrec@1 85.156 (85.156)\n",
      "Validation result: Loss: 0.4873115217208862, Acc: 83.75\n",
      "\n",
      "Epoch: [82][0/391]\tLoss 0.3364 (0.3364)\n",
      "Epoch: [82][100/391]\tLoss 0.3603 (0.3179)\n",
      "Epoch: [82][200/391]\tLoss 0.3025 (0.3276)\n",
      "Epoch: [82][300/391]\tLoss 0.2637 (0.3281)\n",
      "train result: Loss: 0.33036125657081605, Acc: 88.354\n",
      "\n",
      "Test: [0/79]\tLoss 0.4087 (0.4087)\tPrec@1 86.719 (86.719)\n",
      "Validation result: Loss: 0.49401291456222535, Acc: 83.7\n",
      "\n",
      "Epoch: [83][0/391]\tLoss 0.3814 (0.3814)\n",
      "Epoch: [83][100/391]\tLoss 0.2037 (0.3198)\n",
      "Epoch: [83][200/391]\tLoss 0.3922 (0.3285)\n",
      "Epoch: [83][300/391]\tLoss 0.2574 (0.3309)\n",
      "train result: Loss: 0.33423619819641115, Acc: 88.254\n",
      "\n",
      "Test: [0/79]\tLoss 0.6657 (0.6657)\tPrec@1 81.250 (81.250)\n",
      "Validation result: Loss: 0.47293365302085877, Acc: 84.46\n",
      "\n",
      "Epoch: [84][0/391]\tLoss 0.2397 (0.2397)\n",
      "Epoch: [84][100/391]\tLoss 0.2970 (0.3050)\n",
      "Epoch: [84][200/391]\tLoss 0.3265 (0.3116)\n",
      "Epoch: [84][300/391]\tLoss 0.4119 (0.3184)\n",
      "train result: Loss: 0.3209946587085724, Acc: 88.844\n",
      "\n",
      "Test: [0/79]\tLoss 0.4892 (0.4892)\tPrec@1 85.938 (85.938)\n",
      "Validation result: Loss: 0.5112089077711105, Acc: 83.38\n",
      "\n",
      "Epoch: [85][0/391]\tLoss 0.2997 (0.2997)\n",
      "Epoch: [85][100/391]\tLoss 0.3151 (0.3215)\n",
      "Epoch: [85][200/391]\tLoss 0.4485 (0.3229)\n",
      "Epoch: [85][300/391]\tLoss 0.3984 (0.3252)\n",
      "train result: Loss: 0.3283545897293091, Acc: 88.672\n",
      "\n",
      "Test: [0/79]\tLoss 0.5422 (0.5422)\tPrec@1 82.031 (82.031)\n",
      "Validation result: Loss: 0.49044842965602875, Acc: 83.74\n",
      "\n",
      "Epoch: [86][0/391]\tLoss 0.3429 (0.3429)\n",
      "Epoch: [86][100/391]\tLoss 0.5767 (0.3193)\n",
      "Epoch: [86][200/391]\tLoss 0.3661 (0.3198)\n",
      "Epoch: [86][300/391]\tLoss 0.2858 (0.3214)\n",
      "train result: Loss: 0.32646756509780883, Acc: 88.738\n",
      "\n",
      "Test: [0/79]\tLoss 0.4104 (0.4104)\tPrec@1 83.594 (83.594)\n",
      "Validation result: Loss: 0.4878222430706024, Acc: 84.11\n",
      "\n",
      "Epoch: [87][0/391]\tLoss 0.2767 (0.2767)\n",
      "Epoch: [87][100/391]\tLoss 0.3207 (0.3132)\n",
      "Epoch: [87][200/391]\tLoss 0.4308 (0.3201)\n",
      "Epoch: [87][300/391]\tLoss 0.1813 (0.3221)\n",
      "train result: Loss: 0.32567710392951965, Acc: 88.81\n",
      "\n",
      "Test: [0/79]\tLoss 0.7203 (0.7203)\tPrec@1 77.344 (77.344)\n",
      "Validation result: Loss: 0.49114584512710574, Acc: 83.94\n",
      "\n",
      "Epoch: [88][0/391]\tLoss 0.3524 (0.3524)\n",
      "Epoch: [88][100/391]\tLoss 0.2555 (0.3069)\n",
      "Epoch: [88][200/391]\tLoss 0.3033 (0.3121)\n",
      "Epoch: [88][300/391]\tLoss 0.3853 (0.3211)\n",
      "train result: Loss: 0.3222870503234863, Acc: 88.798\n",
      "\n",
      "Test: [0/79]\tLoss 0.5287 (0.5287)\tPrec@1 81.250 (81.250)\n",
      "Validation result: Loss: 0.4802020071983337, Acc: 84.45\n",
      "\n",
      "Epoch: [89][0/391]\tLoss 0.2327 (0.2327)\n",
      "Epoch: [89][100/391]\tLoss 0.2064 (0.3051)\n",
      "Epoch: [89][200/391]\tLoss 0.3293 (0.3094)\n",
      "Epoch: [89][300/391]\tLoss 0.3045 (0.3096)\n",
      "train result: Loss: 0.31910796796798707, Acc: 89.038\n",
      "\n",
      "Test: [0/79]\tLoss 0.8064 (0.8064)\tPrec@1 75.781 (75.781)\n",
      "Validation result: Loss: 0.5026578472614288, Acc: 84.02\n",
      "\n",
      "Epoch: [90][0/391]\tLoss 0.2874 (0.2874)\n",
      "Epoch: [90][100/391]\tLoss 0.3768 (0.3137)\n",
      "Epoch: [90][200/391]\tLoss 0.3323 (0.3149)\n",
      "Epoch: [90][300/391]\tLoss 0.2649 (0.3163)\n",
      "train result: Loss: 0.32025261887550355, Acc: 88.87\n",
      "\n",
      "Test: [0/79]\tLoss 0.5036 (0.5036)\tPrec@1 82.812 (82.812)\n",
      "Validation result: Loss: 0.4644850939273834, Acc: 84.69\n",
      "\n",
      "Epoch: [91][0/391]\tLoss 0.3371 (0.3371)\n",
      "Epoch: [91][100/391]\tLoss 0.2553 (0.3223)\n",
      "Epoch: [91][200/391]\tLoss 0.3470 (0.3151)\n",
      "Epoch: [91][300/391]\tLoss 0.3986 (0.3136)\n",
      "train result: Loss: 0.3157952734184265, Acc: 89.052\n",
      "\n",
      "Test: [0/79]\tLoss 0.5298 (0.5298)\tPrec@1 84.375 (84.375)\n",
      "Validation result: Loss: 0.48478907012939454, Acc: 84.09\n",
      "\n",
      "Epoch: [92][0/391]\tLoss 0.2381 (0.2381)\n",
      "Epoch: [92][100/391]\tLoss 0.2305 (0.2895)\n",
      "Epoch: [92][200/391]\tLoss 0.1973 (0.3042)\n",
      "Epoch: [92][300/391]\tLoss 0.2838 (0.3049)\n",
      "train result: Loss: 0.3089973161315918, Acc: 89.188\n",
      "\n",
      "Test: [0/79]\tLoss 0.7059 (0.7059)\tPrec@1 78.906 (78.906)\n",
      "Validation result: Loss: 0.47554868688583374, Acc: 84.38\n",
      "\n",
      "Epoch: [93][0/391]\tLoss 0.2383 (0.2383)\n",
      "Epoch: [93][100/391]\tLoss 0.1481 (0.2950)\n",
      "Epoch: [93][200/391]\tLoss 0.4022 (0.3026)\n",
      "Epoch: [93][300/391]\tLoss 0.2660 (0.3064)\n",
      "train result: Loss: 0.310390359582901, Acc: 89.36\n",
      "\n",
      "Test: [0/79]\tLoss 0.5411 (0.5411)\tPrec@1 82.812 (82.812)\n",
      "Validation result: Loss: 0.4975184026241303, Acc: 83.43\n",
      "\n",
      "Epoch: [94][0/391]\tLoss 0.4508 (0.4508)\n",
      "Epoch: [94][100/391]\tLoss 0.1869 (0.3018)\n",
      "Epoch: [94][200/391]\tLoss 0.3505 (0.3012)\n",
      "Epoch: [94][300/391]\tLoss 0.3260 (0.3078)\n",
      "train result: Loss: 0.308267323217392, Acc: 89.208\n",
      "\n",
      "Test: [0/79]\tLoss 0.6412 (0.6412)\tPrec@1 78.906 (78.906)\n",
      "Validation result: Loss: 0.5035531555175782, Acc: 83.77\n",
      "\n",
      "Epoch: [95][0/391]\tLoss 0.2919 (0.2919)\n",
      "Epoch: [95][100/391]\tLoss 0.3027 (0.2933)\n",
      "Epoch: [95][200/391]\tLoss 0.2519 (0.2970)\n",
      "Epoch: [95][300/391]\tLoss 0.3039 (0.3007)\n",
      "train result: Loss: 0.3031407747268677, Acc: 89.414\n",
      "\n",
      "Test: [0/79]\tLoss 0.7271 (0.7271)\tPrec@1 77.344 (77.344)\n",
      "Validation result: Loss: 0.4873143469333649, Acc: 84.05\n",
      "\n",
      "Epoch: [96][0/391]\tLoss 0.3291 (0.3291)\n",
      "Epoch: [96][100/391]\tLoss 0.2808 (0.2992)\n",
      "Epoch: [96][200/391]\tLoss 0.3574 (0.2975)\n",
      "Epoch: [96][300/391]\tLoss 0.2644 (0.2969)\n",
      "train result: Loss: 0.30109744303703306, Acc: 89.456\n",
      "\n",
      "Test: [0/79]\tLoss 0.6868 (0.6868)\tPrec@1 78.125 (78.125)\n",
      "Validation result: Loss: 0.5015124416351319, Acc: 83.11\n",
      "\n",
      "Epoch: [97][0/391]\tLoss 0.2842 (0.2842)\n",
      "Epoch: [97][100/391]\tLoss 0.2968 (0.2951)\n",
      "Epoch: [97][200/391]\tLoss 0.3540 (0.2917)\n",
      "Epoch: [97][300/391]\tLoss 0.2429 (0.2985)\n",
      "train result: Loss: 0.3007235363006592, Acc: 89.576\n",
      "\n",
      "Test: [0/79]\tLoss 0.4884 (0.4884)\tPrec@1 86.719 (86.719)\n",
      "Validation result: Loss: 0.49574822154045106, Acc: 83.62\n",
      "\n",
      "Epoch: [98][0/391]\tLoss 0.3519 (0.3519)\n",
      "Epoch: [98][100/391]\tLoss 0.2520 (0.2840)\n",
      "Epoch: [98][200/391]\tLoss 0.2257 (0.2867)\n",
      "Epoch: [98][300/391]\tLoss 0.2604 (0.2975)\n",
      "train result: Loss: 0.3002742592525482, Acc: 89.646\n",
      "\n",
      "Test: [0/79]\tLoss 0.6963 (0.6963)\tPrec@1 81.250 (81.250)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation result: Loss: 0.4911652970314026, Acc: 84.26\n",
      "\n",
      "Epoch: [99][0/391]\tLoss 0.3728 (0.3728)\n",
      "Epoch: [99][100/391]\tLoss 0.3701 (0.2927)\n",
      "Epoch: [99][200/391]\tLoss 0.2298 (0.2944)\n",
      "Epoch: [99][300/391]\tLoss 0.2994 (0.3003)\n",
      "train result: Loss: 0.30253484901428224, Acc: 89.33\n",
      "\n",
      "Test: [0/79]\tLoss 0.7428 (0.7428)\tPrec@1 76.562 (76.562)\n",
      "Validation result: Loss: 0.4885120544433594, Acc: 83.97\n",
      "\n",
      "Epoch: [100][0/391]\tLoss 0.2522 (0.2522)\n",
      "Epoch: [100][100/391]\tLoss 0.2468 (0.2796)\n",
      "Epoch: [100][200/391]\tLoss 0.2761 (0.2897)\n",
      "Epoch: [100][300/391]\tLoss 0.3863 (0.2937)\n",
      "train result: Loss: 0.29210719597816465, Acc: 89.724\n",
      "\n",
      "Test: [0/79]\tLoss 0.4407 (0.4407)\tPrec@1 87.500 (87.500)\n",
      "Validation result: Loss: 0.4855432113647461, Acc: 84.29\n",
      "\n",
      "Epoch: [101][0/391]\tLoss 0.2504 (0.2504)\n",
      "Epoch: [101][100/391]\tLoss 0.3716 (0.2852)\n",
      "Epoch: [101][200/391]\tLoss 0.2286 (0.2889)\n",
      "Epoch: [101][300/391]\tLoss 0.2299 (0.2959)\n",
      "train result: Loss: 0.297877843542099, Acc: 89.77\n",
      "\n",
      "Test: [0/79]\tLoss 0.5706 (0.5706)\tPrec@1 83.594 (83.594)\n",
      "Validation result: Loss: 0.48154724531173704, Acc: 84.02\n",
      "\n",
      "Epoch: [102][0/391]\tLoss 0.3341 (0.3341)\n",
      "Epoch: [102][100/391]\tLoss 0.3801 (0.2843)\n",
      "Epoch: [102][200/391]\tLoss 0.3722 (0.2887)\n",
      "Epoch: [102][300/391]\tLoss 0.2649 (0.2904)\n",
      "train result: Loss: 0.2934439200401306, Acc: 89.694\n",
      "\n",
      "Test: [0/79]\tLoss 0.7472 (0.7472)\tPrec@1 75.000 (75.000)\n",
      "Validation result: Loss: 0.47258792045116427, Acc: 84.88\n",
      "\n",
      "Epoch: [103][0/391]\tLoss 0.3506 (0.3506)\n",
      "Epoch: [103][100/391]\tLoss 0.3020 (0.2880)\n",
      "Epoch: [103][200/391]\tLoss 0.4613 (0.2944)\n",
      "Epoch: [103][300/391]\tLoss 0.3453 (0.2932)\n",
      "train result: Loss: 0.2954179570770264, Acc: 89.81\n",
      "\n",
      "Test: [0/79]\tLoss 0.7581 (0.7581)\tPrec@1 79.688 (79.688)\n",
      "Validation result: Loss: 0.48703420424461363, Acc: 84.48\n",
      "\n",
      "Epoch: [104][0/391]\tLoss 0.2066 (0.2066)\n",
      "Epoch: [104][100/391]\tLoss 0.3101 (0.2843)\n",
      "Epoch: [104][200/391]\tLoss 0.2225 (0.2898)\n",
      "Epoch: [104][300/391]\tLoss 0.2262 (0.2936)\n",
      "train result: Loss: 0.2957199715805054, Acc: 89.746\n",
      "\n",
      "Test: [0/79]\tLoss 0.6623 (0.6623)\tPrec@1 77.344 (77.344)\n",
      "Validation result: Loss: 0.47587487297058106, Acc: 84.8\n",
      "\n",
      "Epoch: [105][0/391]\tLoss 0.2472 (0.2472)\n",
      "Epoch: [105][100/391]\tLoss 0.3082 (0.2866)\n",
      "Epoch: [105][200/391]\tLoss 0.2775 (0.2854)\n",
      "Epoch: [105][300/391]\tLoss 0.3522 (0.2859)\n",
      "train result: Loss: 0.28989618399620054, Acc: 89.928\n",
      "\n",
      "Test: [0/79]\tLoss 0.6230 (0.6230)\tPrec@1 79.688 (79.688)\n",
      "Validation result: Loss: 0.4992858769893646, Acc: 83.71\n",
      "\n",
      "Epoch: [106][0/391]\tLoss 0.2917 (0.2917)\n",
      "Epoch: [106][100/391]\tLoss 0.2624 (0.2792)\n",
      "Epoch: [106][200/391]\tLoss 0.3304 (0.2823)\n",
      "Epoch: [106][300/391]\tLoss 0.3008 (0.2871)\n",
      "train result: Loss: 0.2887866536140442, Acc: 89.97\n",
      "\n",
      "Test: [0/79]\tLoss 0.7826 (0.7826)\tPrec@1 70.312 (70.312)\n",
      "Validation result: Loss: 0.4866298343658447, Acc: 84.21\n",
      "\n",
      "Epoch: [107][0/391]\tLoss 0.2834 (0.2834)\n",
      "Epoch: [107][100/391]\tLoss 0.3431 (0.2845)\n",
      "Epoch: [107][200/391]\tLoss 0.3829 (0.2835)\n",
      "Epoch: [107][300/391]\tLoss 0.2193 (0.2864)\n",
      "train result: Loss: 0.29166202430725097, Acc: 89.73\n",
      "\n",
      "Test: [0/79]\tLoss 0.4424 (0.4424)\tPrec@1 85.156 (85.156)\n",
      "Validation result: Loss: 0.4956497618675232, Acc: 83.83\n",
      "\n",
      "Epoch: [108][0/391]\tLoss 0.1809 (0.1809)\n",
      "Epoch: [108][100/391]\tLoss 0.2965 (0.2848)\n",
      "Epoch: [108][200/391]\tLoss 0.2676 (0.2840)\n",
      "Epoch: [108][300/391]\tLoss 0.2348 (0.2869)\n",
      "train result: Loss: 0.28901052997112275, Acc: 89.86\n",
      "\n",
      "Test: [0/79]\tLoss 0.6489 (0.6489)\tPrec@1 79.688 (79.688)\n",
      "Validation result: Loss: 0.49957952117919924, Acc: 84.0\n",
      "\n",
      "Epoch: [109][0/391]\tLoss 0.3686 (0.3686)\n",
      "Epoch: [109][100/391]\tLoss 0.1959 (0.2923)\n",
      "Epoch: [109][200/391]\tLoss 0.4121 (0.2872)\n",
      "Epoch: [109][300/391]\tLoss 0.3104 (0.2879)\n",
      "train result: Loss: 0.291837615480423, Acc: 89.796\n",
      "\n",
      "Test: [0/79]\tLoss 0.5750 (0.5750)\tPrec@1 79.688 (79.688)\n",
      "Validation result: Loss: 0.46880640335083007, Acc: 84.38\n",
      "\n",
      "Epoch: [110][0/391]\tLoss 0.3997 (0.3997)\n",
      "Epoch: [110][100/391]\tLoss 0.2401 (0.2684)\n",
      "Epoch: [110][200/391]\tLoss 0.1725 (0.2740)\n",
      "Epoch: [110][300/391]\tLoss 0.2643 (0.2775)\n",
      "train result: Loss: 0.28425386702537536, Acc: 90.1\n",
      "\n",
      "Test: [0/79]\tLoss 0.6059 (0.6059)\tPrec@1 80.469 (80.469)\n",
      "Validation result: Loss: 0.47855001726150515, Acc: 84.46\n",
      "\n",
      "Epoch: [111][0/391]\tLoss 0.2330 (0.2330)\n",
      "Epoch: [111][100/391]\tLoss 0.2583 (0.2774)\n",
      "Epoch: [111][200/391]\tLoss 0.2366 (0.2778)\n",
      "Epoch: [111][300/391]\tLoss 0.2139 (0.2782)\n",
      "train result: Loss: 0.2810876900291443, Acc: 90.11\n",
      "\n",
      "Test: [0/79]\tLoss 0.7959 (0.7959)\tPrec@1 75.000 (75.000)\n",
      "Validation result: Loss: 0.4722562143087387, Acc: 84.49\n",
      "\n",
      "Epoch: [112][0/391]\tLoss 0.2528 (0.2528)\n",
      "Epoch: [112][100/391]\tLoss 0.3455 (0.2791)\n",
      "Epoch: [112][200/391]\tLoss 0.2796 (0.2757)\n",
      "Epoch: [112][300/391]\tLoss 0.2739 (0.2794)\n",
      "train result: Loss: 0.28091595708847045, Acc: 90.218\n",
      "\n",
      "Test: [0/79]\tLoss 0.7536 (0.7536)\tPrec@1 75.781 (75.781)\n",
      "Validation result: Loss: 0.5029998049259186, Acc: 83.66\n",
      "\n",
      "Epoch: [113][0/391]\tLoss 0.1785 (0.1785)\n",
      "Epoch: [113][100/391]\tLoss 0.2013 (0.2776)\n",
      "Epoch: [113][200/391]\tLoss 0.3597 (0.2760)\n",
      "Epoch: [113][300/391]\tLoss 0.2481 (0.2848)\n",
      "train result: Loss: 0.2842966477966309, Acc: 89.962\n",
      "\n",
      "Test: [0/79]\tLoss 0.7992 (0.7992)\tPrec@1 75.000 (75.000)\n",
      "Validation result: Loss: 0.475413090133667, Acc: 84.98\n",
      "\n",
      "Epoch: [114][0/391]\tLoss 0.2228 (0.2228)\n",
      "Epoch: [114][100/391]\tLoss 0.2190 (0.2764)\n",
      "Epoch: [114][200/391]\tLoss 0.3324 (0.2798)\n",
      "Epoch: [114][300/391]\tLoss 0.1785 (0.2769)\n",
      "train result: Loss: 0.2770670504283905, Acc: 90.272\n",
      "\n",
      "Test: [0/79]\tLoss 0.4242 (0.4242)\tPrec@1 84.375 (84.375)\n",
      "Validation result: Loss: 0.49531969242095947, Acc: 83.92\n",
      "\n",
      "Epoch: [115][0/391]\tLoss 0.3678 (0.3678)\n",
      "Epoch: [115][100/391]\tLoss 0.2495 (0.2718)\n",
      "Epoch: [115][200/391]\tLoss 0.2934 (0.2650)\n",
      "Epoch: [115][300/391]\tLoss 0.2629 (0.2664)\n",
      "train result: Loss: 0.2732707610273361, Acc: 90.438\n",
      "\n",
      "Test: [0/79]\tLoss 0.8501 (0.8501)\tPrec@1 71.875 (71.875)\n",
      "Validation result: Loss: 0.5087837021827698, Acc: 83.58\n",
      "\n",
      "Epoch: [116][0/391]\tLoss 0.2393 (0.2393)\n",
      "Epoch: [116][100/391]\tLoss 0.2404 (0.2786)\n",
      "Epoch: [116][200/391]\tLoss 0.2287 (0.2743)\n",
      "Epoch: [116][300/391]\tLoss 0.3759 (0.2736)\n",
      "train result: Loss: 0.2767159310054779, Acc: 90.434\n",
      "\n",
      "Test: [0/79]\tLoss 0.4403 (0.4403)\tPrec@1 85.938 (85.938)\n",
      "Validation result: Loss: 0.46703898696899415, Acc: 84.99\n",
      "\n",
      "Epoch: [117][0/391]\tLoss 0.1210 (0.1210)\n",
      "Epoch: [117][100/391]\tLoss 0.2496 (0.2716)\n",
      "Epoch: [117][200/391]\tLoss 0.2083 (0.2759)\n",
      "Epoch: [117][300/391]\tLoss 0.3312 (0.2750)\n",
      "train result: Loss: 0.277467127904892, Acc: 90.294\n",
      "\n",
      "Test: [0/79]\tLoss 0.4687 (0.4687)\tPrec@1 86.719 (86.719)\n",
      "Validation result: Loss: 0.4591398482322693, Acc: 85.19\n",
      "\n",
      "Epoch: [118][0/391]\tLoss 0.2332 (0.2332)\n",
      "Epoch: [118][100/391]\tLoss 0.1823 (0.2730)\n",
      "Epoch: [118][200/391]\tLoss 0.3038 (0.2750)\n",
      "Epoch: [118][300/391]\tLoss 0.2475 (0.2777)\n",
      "train result: Loss: 0.27871422830581666, Acc: 90.32\n",
      "\n",
      "Test: [0/79]\tLoss 0.4088 (0.4088)\tPrec@1 86.719 (86.719)\n",
      "Validation result: Loss: 0.5110866447448731, Acc: 84.41\n",
      "\n",
      "Epoch: [119][0/391]\tLoss 0.1827 (0.1827)\n",
      "Epoch: [119][100/391]\tLoss 0.3061 (0.2697)\n",
      "Epoch: [119][200/391]\tLoss 0.3322 (0.2776)\n",
      "Epoch: [119][300/391]\tLoss 0.2465 (0.2767)\n",
      "train result: Loss: 0.27675832708358766, Acc: 90.286\n",
      "\n",
      "Test: [0/79]\tLoss 0.5003 (0.5003)\tPrec@1 82.031 (82.031)\n",
      "Validation result: Loss: 0.5051419986724853, Acc: 83.87\n",
      "\n",
      "Epoch: [120][0/391]\tLoss 0.2142 (0.2142)\n",
      "Epoch: [120][100/391]\tLoss 0.2136 (0.2662)\n",
      "Epoch: [120][200/391]\tLoss 0.2140 (0.2660)\n",
      "Epoch: [120][300/391]\tLoss 0.2982 (0.2691)\n",
      "train result: Loss: 0.27077922300338747, Acc: 90.578\n",
      "\n",
      "Test: [0/79]\tLoss 0.6430 (0.6430)\tPrec@1 78.906 (78.906)\n",
      "Validation result: Loss: 0.4917331152439117, Acc: 83.81\n",
      "\n",
      "Epoch: [121][0/391]\tLoss 0.3536 (0.3536)\n",
      "Epoch: [121][100/391]\tLoss 0.2878 (0.2658)\n",
      "Epoch: [121][200/391]\tLoss 0.3832 (0.2647)\n",
      "Epoch: [121][300/391]\tLoss 0.2524 (0.2715)\n",
      "train result: Loss: 0.271428617143631, Acc: 90.508\n",
      "\n",
      "Test: [0/79]\tLoss 0.4465 (0.4465)\tPrec@1 85.156 (85.156)\n",
      "Validation result: Loss: 0.494597621011734, Acc: 83.9\n",
      "\n",
      "Epoch: [122][0/391]\tLoss 0.3048 (0.3048)\n",
      "Epoch: [122][100/391]\tLoss 0.2497 (0.2613)\n",
      "Epoch: [122][200/391]\tLoss 0.2508 (0.2703)\n",
      "Epoch: [122][300/391]\tLoss 0.2243 (0.2749)\n",
      "train result: Loss: 0.27001977434158325, Acc: 90.744\n",
      "\n",
      "Test: [0/79]\tLoss 0.6720 (0.6720)\tPrec@1 82.031 (82.031)\n",
      "Validation result: Loss: 0.4904725543022156, Acc: 84.56\n",
      "\n",
      "Epoch: [123][0/391]\tLoss 0.2581 (0.2581)\n",
      "Epoch: [123][100/391]\tLoss 0.3369 (0.2588)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [123][200/391]\tLoss 0.2507 (0.2622)\n",
      "Epoch: [123][300/391]\tLoss 0.3941 (0.2697)\n",
      "train result: Loss: 0.27062490075826645, Acc: 90.518\n",
      "\n",
      "Test: [0/79]\tLoss 0.5536 (0.5536)\tPrec@1 84.375 (84.375)\n",
      "Validation result: Loss: 0.4655390938282013, Acc: 85.19\n",
      "\n",
      "Epoch: [124][0/391]\tLoss 0.2767 (0.2767)\n",
      "Epoch: [124][100/391]\tLoss 0.3667 (0.2580)\n",
      "Epoch: [124][200/391]\tLoss 0.2605 (0.2565)\n",
      "Epoch: [124][300/391]\tLoss 0.3472 (0.2609)\n",
      "train result: Loss: 0.2650557202911377, Acc: 90.738\n",
      "\n",
      "Test: [0/79]\tLoss 0.7051 (0.7051)\tPrec@1 78.125 (78.125)\n",
      "Validation result: Loss: 0.46299514904022215, Acc: 84.81\n",
      "\n",
      "Epoch: [125][0/391]\tLoss 0.2853 (0.2853)\n",
      "Epoch: [125][100/391]\tLoss 0.2602 (0.2562)\n",
      "Epoch: [125][200/391]\tLoss 0.3551 (0.2576)\n",
      "Epoch: [125][300/391]\tLoss 0.2909 (0.2611)\n",
      "train result: Loss: 0.2631544610881805, Acc: 90.9\n",
      "\n",
      "Test: [0/79]\tLoss 0.6002 (0.6002)\tPrec@1 80.469 (80.469)\n",
      "Validation result: Loss: 0.47678618149757385, Acc: 84.85\n",
      "\n",
      "Epoch: [126][0/391]\tLoss 0.2323 (0.2323)\n",
      "Epoch: [126][100/391]\tLoss 0.2232 (0.2559)\n",
      "Epoch: [126][200/391]\tLoss 0.1273 (0.2543)\n",
      "Epoch: [126][300/391]\tLoss 0.3011 (0.2594)\n",
      "train result: Loss: 0.2642445975780487, Acc: 90.686\n",
      "\n",
      "Test: [0/79]\tLoss 0.5290 (0.5290)\tPrec@1 78.906 (78.906)\n",
      "Validation result: Loss: 0.49532373342514036, Acc: 84.42\n",
      "\n",
      "Epoch: [127][0/391]\tLoss 0.2551 (0.2551)\n",
      "Epoch: [127][100/391]\tLoss 0.2033 (0.2603)\n",
      "Epoch: [127][200/391]\tLoss 0.1858 (0.2699)\n",
      "Epoch: [127][300/391]\tLoss 0.2446 (0.2644)\n",
      "train result: Loss: 0.26484625056266786, Acc: 90.934\n",
      "\n",
      "Test: [0/79]\tLoss 0.6091 (0.6091)\tPrec@1 84.375 (84.375)\n",
      "Validation result: Loss: 0.5052386121749878, Acc: 84.18\n",
      "\n",
      "Epoch: [128][0/391]\tLoss 0.2292 (0.2292)\n",
      "Epoch: [128][100/391]\tLoss 0.1463 (0.2550)\n",
      "Epoch: [128][200/391]\tLoss 0.3128 (0.2599)\n",
      "Epoch: [128][300/391]\tLoss 0.3736 (0.2658)\n",
      "train result: Loss: 0.26824041507720947, Acc: 90.632\n",
      "\n",
      "Test: [0/79]\tLoss 0.4754 (0.4754)\tPrec@1 85.938 (85.938)\n",
      "Validation result: Loss: 0.478700083065033, Acc: 84.65\n",
      "\n",
      "Epoch: [129][0/391]\tLoss 0.2820 (0.2820)\n",
      "Epoch: [129][100/391]\tLoss 0.3218 (0.2658)\n",
      "Epoch: [129][200/391]\tLoss 0.2367 (0.2608)\n",
      "Epoch: [129][300/391]\tLoss 0.2763 (0.2641)\n",
      "train result: Loss: 0.26291665417671206, Acc: 90.884\n",
      "\n",
      "Test: [0/79]\tLoss 0.4971 (0.4971)\tPrec@1 85.156 (85.156)\n",
      "Validation result: Loss: 0.4854116973400116, Acc: 84.8\n",
      "\n",
      "Epoch: [130][0/391]\tLoss 0.2498 (0.2498)\n",
      "Epoch: [130][100/391]\tLoss 0.2308 (0.2511)\n",
      "Epoch: [130][200/391]\tLoss 0.2286 (0.2576)\n",
      "Epoch: [130][300/391]\tLoss 0.2068 (0.2572)\n",
      "train result: Loss: 0.25973063282966613, Acc: 90.996\n",
      "\n",
      "Test: [0/79]\tLoss 0.4346 (0.4346)\tPrec@1 86.719 (86.719)\n",
      "Validation result: Loss: 0.46825345406532287, Acc: 84.64\n",
      "\n",
      "Epoch: [131][0/391]\tLoss 0.2811 (0.2811)\n",
      "Epoch: [131][100/391]\tLoss 0.1612 (0.2526)\n",
      "Epoch: [131][200/391]\tLoss 0.2311 (0.2627)\n",
      "Epoch: [131][300/391]\tLoss 0.4001 (0.2617)\n",
      "train result: Loss: 0.2620471483516693, Acc: 90.938\n",
      "\n",
      "Test: [0/79]\tLoss 0.5299 (0.5299)\tPrec@1 82.812 (82.812)\n",
      "Validation result: Loss: 0.48091224188804627, Acc: 85.22\n",
      "\n",
      "Epoch: [132][0/391]\tLoss 0.2574 (0.2574)\n",
      "Epoch: [132][100/391]\tLoss 0.1980 (0.2446)\n",
      "Epoch: [132][200/391]\tLoss 0.2408 (0.2472)\n",
      "Epoch: [132][300/391]\tLoss 0.2118 (0.2532)\n",
      "train result: Loss: 0.2558628688240051, Acc: 91.148\n",
      "\n",
      "Test: [0/79]\tLoss 0.4170 (0.4170)\tPrec@1 85.938 (85.938)\n",
      "Validation result: Loss: 0.47395615382194517, Acc: 84.54\n",
      "\n",
      "Epoch: [133][0/391]\tLoss 0.2156 (0.2156)\n",
      "Epoch: [133][100/391]\tLoss 0.2736 (0.2476)\n",
      "Epoch: [133][200/391]\tLoss 0.1184 (0.2547)\n",
      "Epoch: [133][300/391]\tLoss 0.3297 (0.2592)\n",
      "train result: Loss: 0.2638475020217895, Acc: 90.756\n",
      "\n",
      "Test: [0/79]\tLoss 0.4492 (0.4492)\tPrec@1 83.594 (83.594)\n",
      "Validation result: Loss: 0.4593420922994614, Acc: 85.29\n",
      "\n",
      "Epoch: [134][0/391]\tLoss 0.2731 (0.2731)\n",
      "Epoch: [134][100/391]\tLoss 0.3054 (0.2548)\n",
      "Epoch: [134][200/391]\tLoss 0.3194 (0.2562)\n",
      "Epoch: [134][300/391]\tLoss 0.2152 (0.2505)\n",
      "train result: Loss: 0.25249934329986573, Acc: 91.292\n",
      "\n",
      "Test: [0/79]\tLoss 0.3493 (0.3493)\tPrec@1 89.062 (89.062)\n",
      "Validation result: Loss: 0.4904600203037262, Acc: 84.35\n",
      "\n",
      "Epoch: [135][0/391]\tLoss 0.3870 (0.3870)\n",
      "Epoch: [135][100/391]\tLoss 0.3517 (0.2659)\n",
      "Epoch: [135][200/391]\tLoss 0.1937 (0.2639)\n",
      "Epoch: [135][300/391]\tLoss 0.2688 (0.2626)\n",
      "train result: Loss: 0.26229354780197145, Acc: 90.97\n",
      "\n",
      "Test: [0/79]\tLoss 0.4450 (0.4450)\tPrec@1 82.812 (82.812)\n",
      "Validation result: Loss: 0.4863925232410431, Acc: 84.73\n",
      "\n",
      "Epoch: [136][0/391]\tLoss 0.2893 (0.2893)\n",
      "Epoch: [136][100/391]\tLoss 0.2715 (0.2611)\n",
      "Epoch: [136][200/391]\tLoss 0.2986 (0.2560)\n",
      "Epoch: [136][300/391]\tLoss 0.3404 (0.2558)\n",
      "train result: Loss: 0.255446645655632, Acc: 91.214\n",
      "\n",
      "Test: [0/79]\tLoss 0.5719 (0.5719)\tPrec@1 82.812 (82.812)\n",
      "Validation result: Loss: 0.4890971128463745, Acc: 84.43\n",
      "\n",
      "Epoch: [137][0/391]\tLoss 0.2150 (0.2150)\n",
      "Epoch: [137][100/391]\tLoss 0.3344 (0.2675)\n",
      "Epoch: [137][200/391]\tLoss 0.2931 (0.2678)\n",
      "Epoch: [137][300/391]\tLoss 0.3096 (0.2627)\n",
      "train result: Loss: 0.263105358581543, Acc: 90.864\n",
      "\n",
      "Test: [0/79]\tLoss 0.7099 (0.7099)\tPrec@1 77.344 (77.344)\n",
      "Validation result: Loss: 0.49317787528038026, Acc: 84.03\n",
      "\n",
      "Epoch: [138][0/391]\tLoss 0.2209 (0.2209)\n",
      "Epoch: [138][100/391]\tLoss 0.1940 (0.2473)\n",
      "Epoch: [138][200/391]\tLoss 0.2436 (0.2544)\n",
      "Epoch: [138][300/391]\tLoss 0.2289 (0.2550)\n",
      "train result: Loss: 0.2575971492099762, Acc: 91.038\n",
      "\n",
      "Test: [0/79]\tLoss 0.5320 (0.5320)\tPrec@1 82.031 (82.031)\n",
      "Validation result: Loss: 0.46750301012992856, Acc: 84.85\n",
      "\n",
      "Epoch: [139][0/391]\tLoss 0.2822 (0.2822)\n",
      "Epoch: [139][100/391]\tLoss 0.2539 (0.2661)\n",
      "Epoch: [139][200/391]\tLoss 0.2835 (0.2572)\n",
      "Epoch: [139][300/391]\tLoss 0.3020 (0.2598)\n",
      "train result: Loss: 0.258602677192688, Acc: 90.916\n",
      "\n",
      "Test: [0/79]\tLoss 0.5793 (0.5793)\tPrec@1 79.688 (79.688)\n",
      "Validation result: Loss: 0.4634854727268219, Acc: 85.55\n",
      "\n",
      "Epoch: [140][0/391]\tLoss 0.1438 (0.1438)\n",
      "Epoch: [140][100/391]\tLoss 0.2293 (0.2494)\n",
      "Epoch: [140][200/391]\tLoss 0.2464 (0.2560)\n",
      "Epoch: [140][300/391]\tLoss 0.2554 (0.2545)\n",
      "train result: Loss: 0.25420215666770934, Acc: 91.154\n",
      "\n",
      "Test: [0/79]\tLoss 0.6215 (0.6215)\tPrec@1 79.688 (79.688)\n",
      "Validation result: Loss: 0.4592774306297302, Acc: 85.22\n",
      "\n",
      "Epoch: [141][0/391]\tLoss 0.3282 (0.3282)\n",
      "Epoch: [141][100/391]\tLoss 0.1752 (0.2395)\n",
      "Epoch: [141][200/391]\tLoss 0.2753 (0.2460)\n",
      "Epoch: [141][300/391]\tLoss 0.2780 (0.2486)\n",
      "train result: Loss: 0.25259419141769407, Acc: 91.172\n",
      "\n",
      "Test: [0/79]\tLoss 0.6040 (0.6040)\tPrec@1 79.688 (79.688)\n",
      "Validation result: Loss: 0.4682498230934143, Acc: 85.13\n",
      "\n",
      "Epoch: [142][0/391]\tLoss 0.3276 (0.3276)\n",
      "Epoch: [142][100/391]\tLoss 0.2051 (0.2508)\n",
      "Epoch: [142][200/391]\tLoss 0.3548 (0.2488)\n",
      "Epoch: [142][300/391]\tLoss 0.2624 (0.2529)\n",
      "train result: Loss: 0.2511043372297287, Acc: 91.298\n",
      "\n",
      "Test: [0/79]\tLoss 0.5506 (0.5506)\tPrec@1 79.688 (79.688)\n",
      "Validation result: Loss: 0.46225374112129214, Acc: 85.41\n",
      "\n",
      "Epoch: [143][0/391]\tLoss 0.2463 (0.2463)\n",
      "Epoch: [143][100/391]\tLoss 0.2305 (0.2526)\n",
      "Epoch: [143][200/391]\tLoss 0.1671 (0.2520)\n",
      "Epoch: [143][300/391]\tLoss 0.2056 (0.2516)\n",
      "train result: Loss: 0.2526815551185608, Acc: 91.202\n",
      "\n",
      "Test: [0/79]\tLoss 0.6320 (0.6320)\tPrec@1 78.125 (78.125)\n",
      "Validation result: Loss: 0.4600278817176819, Acc: 84.96\n",
      "\n",
      "Epoch: [144][0/391]\tLoss 0.2335 (0.2335)\n",
      "Epoch: [144][100/391]\tLoss 0.3367 (0.2531)\n",
      "Epoch: [144][200/391]\tLoss 0.1598 (0.2555)\n",
      "Epoch: [144][300/391]\tLoss 0.2762 (0.2606)\n",
      "train result: Loss: 0.2598966163253784, Acc: 90.882\n",
      "\n",
      "Test: [0/79]\tLoss 0.7843 (0.7843)\tPrec@1 78.125 (78.125)\n",
      "Validation result: Loss: 0.46684527106285095, Acc: 85.34\n",
      "\n",
      "Epoch: [145][0/391]\tLoss 0.2235 (0.2235)\n",
      "Epoch: [145][100/391]\tLoss 0.2164 (0.2436)\n",
      "Epoch: [145][200/391]\tLoss 0.3224 (0.2444)\n",
      "Epoch: [145][300/391]\tLoss 0.1621 (0.2459)\n",
      "train result: Loss: 0.249392746591568, Acc: 91.578\n",
      "\n",
      "Test: [0/79]\tLoss 0.7786 (0.7786)\tPrec@1 76.562 (76.562)\n",
      "Validation result: Loss: 0.4691109293937683, Acc: 84.73\n",
      "\n",
      "Epoch: [146][0/391]\tLoss 0.1891 (0.1891)\n",
      "Epoch: [146][100/391]\tLoss 0.2344 (0.2286)\n",
      "Epoch: [146][200/391]\tLoss 0.2079 (0.2353)\n",
      "Epoch: [146][300/391]\tLoss 0.2798 (0.2442)\n",
      "train result: Loss: 0.24467039184093475, Acc: 91.546\n",
      "\n",
      "Test: [0/79]\tLoss 0.6522 (0.6522)\tPrec@1 82.031 (82.031)\n",
      "Validation result: Loss: 0.4668847725868225, Acc: 85.1\n",
      "\n",
      "Epoch: [147][0/391]\tLoss 0.2408 (0.2408)\n",
      "Epoch: [147][100/391]\tLoss 0.2083 (0.2414)\n",
      "Epoch: [147][200/391]\tLoss 0.1896 (0.2504)\n",
      "Epoch: [147][300/391]\tLoss 0.2811 (0.2503)\n",
      "train result: Loss: 0.25148197130680083, Acc: 91.156\n",
      "\n",
      "Test: [0/79]\tLoss 0.6823 (0.6823)\tPrec@1 78.906 (78.906)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation result: Loss: 0.45717708282470704, Acc: 85.51\n",
      "\n",
      "Epoch: [148][0/391]\tLoss 0.2837 (0.2837)\n",
      "Epoch: [148][100/391]\tLoss 0.2322 (0.2421)\n",
      "Epoch: [148][200/391]\tLoss 0.3173 (0.2466)\n",
      "Epoch: [148][300/391]\tLoss 0.2579 (0.2509)\n",
      "train result: Loss: 0.25018110598564147, Acc: 91.31\n",
      "\n",
      "Test: [0/79]\tLoss 0.4884 (0.4884)\tPrec@1 86.719 (86.719)\n",
      "Validation result: Loss: 0.45929483036994934, Acc: 85.47\n",
      "\n",
      "Epoch: [149][0/391]\tLoss 0.1701 (0.1701)\n",
      "Epoch: [149][100/391]\tLoss 0.2552 (0.2370)\n",
      "Epoch: [149][200/391]\tLoss 0.3371 (0.2470)\n",
      "Epoch: [149][300/391]\tLoss 0.2692 (0.2500)\n",
      "train result: Loss: 0.25227845705986024, Acc: 91.31\n",
      "\n",
      "Test: [0/79]\tLoss 0.9047 (0.9047)\tPrec@1 73.438 (73.438)\n",
      "Validation result: Loss: 0.49257507467269895, Acc: 84.61\n",
      "\n"
     ]
    }
   ],
   "source": [
    "GPU_NUM = 0\n",
    "lr = 1e-2\n",
    "batch_size = 128\n",
    "epochs = 150\n",
    "num_classes = 10\n",
    "\n",
    "device = torch.device(f'cuda:{GPU_NUM}' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "train_dataset = dataset(is_train=True)\n",
    "val_dataset = dataset(is_train=False)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=False)\n",
    "\n",
    "\n",
    "model = CNN_model(num_classes=num_classes).to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "train_loss_arr = []\n",
    "train_acc_arr = []\n",
    "\n",
    "val_loss_arr = []\n",
    "val_acc_arr = []\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    for i, (data, target) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        \n",
    "        target = target.to(device)\n",
    "\n",
    "        output = model(data) \n",
    "\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        output.float()\n",
    "        loss.float()\n",
    "\n",
    "        prec1 = accuracy(output.data, target)\n",
    "        prec1 = prec1[0]\n",
    "\n",
    "        losses.update(loss.item(), data.size(0))\n",
    "        top1.update(prec1.item(), data.size(0))\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})'.format(epoch, i, len(train_loader), loss=losses))\n",
    "\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss_arr.append(losses.avg)\n",
    "    train_acc_arr.append(top1.avg)\n",
    "    print(\"train result: Loss: {}, Acc: {}\\n\".format(losses.avg, top1.avg))\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss_sum = 0\n",
    "        val_acc_sum = 0\n",
    "\n",
    "        losses = AverageMeter()\n",
    "        top1 = AverageMeter()\n",
    "\n",
    "        for i, (data, target) in enumerate(val_loader):\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            output = model(data) \n",
    "\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            output.float()\n",
    "            loss.float()\n",
    "\n",
    "            prec1 = accuracy(output.data, target)\n",
    "\n",
    "            prec1 = prec1[0]\n",
    "            \n",
    "            losses.update(loss.item(), data.size(0))\n",
    "            top1.update(prec1.item(), data.size(0))\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print('Test: [{0}/{1}]\\t'\n",
    "                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                      'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
    "                          i, len(val_loader), loss=losses, top1=top1))\n",
    "\n",
    "        val_loss_arr.append(losses.avg)\n",
    "        val_acc_arr.append(top1.avg)\n",
    "        print(\"Validation result: Loss: {}, Acc: {}\\n\".format(losses.avg, top1.avg))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max(train_acc) 91.578 max(val_acc) 85.55"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4. (Compare FCN model and CNN model)\n",
    "- compare two model's performance\n",
    "- subscribe your comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My opinion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Note]\n",
    "* To make the comparison fair, both FCN model and CNN model are consisted of **9 layers**:\n",
    "    * FCN Model: 9 = 9 Fully-connected Layers (+ 4 Drop-out)\n",
    "    * CNN Model: 9 = 5 Convolutional Layers + 4 Fully-Connected Layers (+ 1 Drop-out).\n",
    "        * Drop-out is not regarded as a layer here.\n",
    "* Also, both models use **ReLU** as a non-linear activation function and trained with **SGD optimizer (learning rate: 1e-2, momentum: 0.9, weight decay:5e-4) in 150 epochs**.\n",
    "* Experiments were conducted on on the **CIFAR-10** which is consisted of `train` and `test` dataset: I use `train` for training and `test` for validation.\n",
    "\n",
    "[Comparison]\n",
    "* As shown in Fig. 1 and Fig. 2, the **CNN model** achieved significant performance improvement of **24.97%** in terms of validation accuracy on CIFAR-10, which implies that learned `Convolution` filters well extract essential features from the input patch.\n",
    "(Also, training accuracy in the CNN model was also improved compared to the FCN model.)\n",
    "\n",
    "[Limitation]\n",
    "* In my thought, the classification accuracy can be improved if I also use spatial `Max-Pooling` layers in my CNN model. (As you know, classic CNNs consist of alternatively stacked `Convolution` filters and `Max-Pooling` layers.)\n",
    "* Also, if I use a learning rate scheduler with more epochs for experiments, the accuracy of both models can be enhanced. But, I still believe that the **CNN model** would beat the **FCN model** because the **CNN model** handles input images considering their spatial structure."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "metadata": {
   "interpreter": {
    "hash": "6927d40f424fed17536fb2a7ae3757b3def1cb1d1a33b1812bfe4ee6a451eed4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
